"""
Spark Consumer Service - Kafka to Spark Streaming Integration

Consumes interferometry visibility data from Kafka using Spark Structured Streaming.
Deserializes chunks and provides basic distributed processing with console output.

This service replaces the traditional Kafka consumer with Spark streaming capabilities
for distributed processing of large-scale interferometry datasets.
"""

import sys
import argparse
from pathlib import Path
import time
from datetime import datetime, timedelta
import msgpack
import numpy as np
import zlib
import json
import random

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError

# Add src directory to path
project_root = Path(__file__).parent.parent
src_path = project_root / "src"
sys.path.append(str(src_path))

from processing.spark_session import create_spark_session
from bda.bda_processor import process_microbatch_with_bda, format_bda_result_for_output, create_bda_summary_stats
from bda.bda_config import load_bda_config_with_fallback, get_default_bda_config


def ensure_kafka_topic_exists(kafka_servers: str, topic: str, num_partitions: int = 4, max_retries: int = 30) -> bool:
    """
    Ensure that a Kafka topic exists, creating it if necessary with robust retry logic.
    
    This function checks if the specified topic exists in Kafka. If it doesn't exist,
    it creates the topic with the specified number of partitions and waits for it to
    be available. This is essential for the streaming architecture to handle topic
    creation before consumers connect.
    
    Parameters
    ----------
    kafka_servers : str
        Kafka bootstrap servers
    topic : str
        Topic name to create/verify
    num_partitions : int
        Number of partitions for the topic (default: 4 for distributed processing)
    max_retries : int
        Maximum number of retries to wait for topic availability (default: 30)
        
    Returns
    -------
    bool
        True if topic exists or was created successfully
    """
    
    try:
        # Create admin client with longer timeout
        admin_client = KafkaAdminClient(
            bootstrap_servers=kafka_servers.split(','),
            client_id='spark_consumer_admin',
            request_timeout_ms=10000,
            connections_max_idle_ms=60000
        )
        
        # Retry loop to ensure topic is available
        for attempt in range(max_retries):
            try:
                # Check if topic exists
                existing_topics = admin_client.list_topics()
                if topic in existing_topics:
                    print(f"‚úÖ Topic '{topic}' is available (attempt {attempt + 1})")
                    return True
                
                # If this is the first attempt, try to create the topic
                if attempt == 0:
                    topic_config = NewTopic(
                        name=topic,
                        num_partitions=num_partitions,
                        replication_factor=1
                    )
                    
                    print(f"üîß Creating topic '{topic}' with {num_partitions} partitions...")
                    admin_client.create_topics([topic_config])
                    print(f"‚úÖ Topic '{topic}' creation initiated")
                
                # Wait before next check
                if attempt < max_retries - 1:
                    print(f"‚è≥ Waiting for topic '{topic}' to be available... (attempt {attempt + 1}/{max_retries})")
                    time.sleep(1)
                    
            except TopicAlreadyExistsError:
                print(f"‚úÖ Topic '{topic}' already exists (concurrent creation)")
                # Still need to verify it's available
                time.sleep(0.5)
                continue
                
            except Exception as e:
                if attempt == 0:
                    print(f"‚ö†Ô∏è  Could not create topic '{topic}': {e}")
                    print(f"üîÑ Will wait for auto-create or manual creation...")
                time.sleep(1)
                continue
        
        print(f"‚ö†Ô∏è  Topic '{topic}' not confirmed available after {max_retries} attempts")
        print(f"üîÑ Proceeding with streaming (relying on Kafka auto-create)")
        return False
        
    except Exception as e:
        print(f"‚ùå Error managing topic '{topic}': {e}")
        print(f"üîÑ Will rely on Kafka auto-create during streaming")
        return False


def wait_for_kafka_ready(kafka_servers: str, timeout: int = 60) -> bool:
    """
    Wait for Kafka to be ready and responsive.
    
    Parameters
    ----------
    kafka_servers : str
        Kafka bootstrap servers
    timeout : int
        Maximum time to wait in seconds
        
    Returns
    -------
    bool
        True if Kafka is ready
    """
    
    print(f"üîç Checking Kafka connectivity to {kafka_servers}...")
    
    try:        
        end_time = datetime.now() + timedelta(seconds=timeout)
        
        while datetime.now() < end_time:
            try:
                admin_client = KafkaAdminClient(
                    bootstrap_servers=kafka_servers.split(','),
                    client_id='kafka_health_check',
                    request_timeout_ms=5000
                )
                
                # Try to list topics as a connectivity test
                topics = admin_client.list_topics()
                print(f"‚úÖ Kafka is ready! Found {len(topics)} topics")
                return True
                
            except Exception as e:
                remaining = int((end_time - datetime.now()).total_seconds())
                if remaining > 0:
                    print(f"‚è≥ Kafka not ready yet... retrying in 2s (timeout: {remaining}s)")
                    time.sleep(2)
                else:
                    break
        
        print(f"‚ùå Kafka not ready after {timeout}s timeout")
        return False
        
    except Exception as e:
        print(f"‚ùå Cannot check Kafka connectivity: {e}")
        return False


def deserialize_chunk_data(raw_data_bytes: bytes) -> dict:
    """
    Deserialize MessagePack chunk data from Kafka - FASE 2 Version.
    
    Deserializes complete chunk with all arrays for row decomposition.
    
    Parameters
    ----------
    raw_data_bytes : bytes
        Raw MessagePack serialized data from Kafka
        
    Returns
    -------
    dict
        Deserialized chunk with numpy arrays reconstructed
    """
    
    try:
        # Deserialize MessagePack
        msgpack_data = msgpack.unpackb(raw_data_bytes, raw=False, strict_map_key=False)
        
        chunk = {}
        for key, value in msgpack_data.items():
            if isinstance(value, dict) and 'type' in value:
                if value['type'] == 'ndarray_compressed':
                    # Decompress and reconstruct array
                    decompressed = zlib.decompress(value['data'])
                    array = np.frombuffer(decompressed, dtype=value['dtype'])
                    chunk[key] = array.reshape(value['shape'])
                elif value['type'] == 'ndarray':
                    # Reconstruct array directly
                    array = np.frombuffer(value['data'], dtype=value['dtype'])
                    chunk[key] = array.reshape(value['shape'])
                else:
                    chunk[key] = value
            else:
                chunk[key] = value
                
        return chunk
        
    except Exception as e:
        print(f"‚ùå Error deserializing chunk: {e}")
        return {}


def decompose_chunk_to_rows(chunk: dict) -> list:
    """
    Descompone un chunk en filas individuales para procesamiento BDA - FASE 2.
    
    Convierte un chunk que contiene m√∫ltiples filas de visibilidades
    en una lista de registros individuales, cada uno representando
    una fila con su baseline_key y scan_number.
    
    Parameters
    ----------
    chunk : dict
        Chunk deserializado con arrays cient√≠ficos
        
    Returns
    -------
    list
        Lista de filas individuales con baseline_key y metadata
    """

    rows = []
    
    try:
        nrows = chunk.get('nrows', 0)
        subms_id = chunk.get('subms_id', 'unknown')
        
        if nrows == 0:
            return rows
            
        # Extraer arrays principales
        antenna1 = chunk.get('antenna1', np.array([]))
        antenna2 = chunk.get('antenna2', np.array([]))
        scan_number = chunk.get('scan_number', np.array([]))
        time_array = chunk.get('time', np.array([]))
        u_array = chunk.get('u', np.array([]))
        v_array = chunk.get('v', np.array([]))
        w_array = chunk.get('w', np.array([]))
        visibilities = chunk.get('visibilities', np.array([]))
        weight = chunk.get('weight', np.array([]))
        flag = chunk.get('flag', np.array([]))
        
        # Validar dimensiones
        if len(antenna1) != nrows or len(antenna2) != nrows:
            print(f"‚ö†Ô∏è  Dimension mismatch in chunk {chunk.get('chunk_id', 'unknown')}")
            return rows
            
        # Descomponer fila por fila
        for row_idx in range(nrows):
            ant1 = int(antenna1[row_idx]) if len(antenna1) > row_idx else -1
            ant2 = int(antenna2[row_idx]) if len(antenna2) > row_idx else -1
            scan_num = int(scan_number[row_idx]) if len(scan_number) > row_idx else -1
            
            # Crear baseline_key normalizado
            baseline_key = normalize_baseline_key(ant1, ant2, subms_id)
            
            # Crear registro de fila individual
            row_record = {
                # Identificadores para agrupaci√≥n
                'baseline_key': baseline_key,
                'scan_number': scan_num,
                'antenna1': ant1,
                'antenna2': ant2,
                'subms_id': subms_id,
                
                # Metadata temporal y espacial
                'time': float(time_array[row_idx]) if len(time_array) > row_idx else 0.0,
                'u': float(u_array[row_idx]) if len(u_array) > row_idx else 0.0,
                'v': float(v_array[row_idx]) if len(v_array) > row_idx else 0.0,
                'w': float(w_array[row_idx]) if len(w_array) > row_idx else 0.0,
                
                # Datos cient√≠ficos reales para BDA
                'visibility': visibilities[row_idx] if len(visibilities) > row_idx else np.array([]),
                'weight': weight[row_idx] if len(weight) > row_idx else np.array([]),
                'flag': flag[row_idx] if len(flag) > row_idx else np.array([]),
                
                # Metadata original del chunk
                'original_chunk_id': chunk.get('chunk_id', -1),
                'row_index_in_chunk': row_idx,
                'field_id': chunk.get('field_id', -1),
                'spw_id': chunk.get('spw_id', -1),
            }
            
            rows.append(row_record)
            
    except Exception as e:
        print(f"‚ùå Error decomposing chunk: {e}")
        
    return rows


def group_rows_by_baseline_scan(rows: list) -> dict:
    """
    Agrupa filas individuales por baseline_key + scan_number - FASE 2.
    
    Parameters
    ----------
    rows : list
        Lista de filas individuales decomposed from chunks
        
    Returns
    -------
    dict
        Diccionario con groups key = (baseline_key, scan_number)
    """
    groups = {}
    
    for row in rows:
        baseline_key = row.get('baseline_key', 'unknown')
        scan_number = row.get('scan_number', -1)
        
        # Crear group key usando funci√≥n consistente
        # Extraer antenna1, antenna2 del baseline_key si es posible
        if 'antenna1' in row and 'antenna2' in row:
            ant1 = row['antenna1']
            ant2 = row['antenna2']
            subms_id = row.get('subms_id', None)
            group_key = create_group_key(ant1, ant2, scan_number, subms_id)
        else:
            # Fallback al m√©todo anterior
            group_key = f"{baseline_key}_scan{scan_number}"
        
        if group_key not in groups:
            groups[group_key] = []
            
        groups[group_key].append(row)
        
    return groups


def log_chunk_summary(chunk_metadata: dict, rows: list) -> None:
    """
    Logging conciso con informaci√≥n esencial del chunk - FASE 2.
    """
    chunk_id = chunk_metadata.get('chunk_id', 'unknown')
    subms_id = chunk_metadata.get('subms_id', 'unknown')
    
    # Calcular estad√≠sticas esenciales
    unique_baselines = set([row.get('baseline_key', 'unknown') for row in rows])
    unique_scans = set([row.get('scan_number', -1) for row in rows])
    times = [row.get('time', 0.0) for row in rows]
    avg_time = sum(times) / len(times) if times else 0.0
    
    print(f"üì¶ Chunk {subms_id}_{chunk_id}: {len(rows)} rows | {len(unique_baselines)} baselines | {len(unique_scans)} scans | time={avg_time:.1f}")


def log_groups_summary(groups: dict) -> None:
    """
    Logging conciso de grupos por baseline + scan - FASE 2.
    """
    total_rows = sum(len(group_rows) for group_rows in groups.values())
    print(f"ÔøΩ Groups: {len(groups)} | Total rows: {total_rows}")


def deserialize_chunk_udf():
    """
    Create UDF for deserializing MessagePack chunks in Spark.
    
    Returns
    -------
    function
        Spark UDF for chunk deserialization
    """
    
    def deserialize_chunk_data(raw_data_bytes):
        """
        Deserialize MessagePack chunk data.
        
        Parameters
        ----------
        raw_data_bytes : bytes
            Raw MessagePack serialized data
            
        Returns
        -------
        str
            JSON string of deserialized chunk metadata
        """
        try:
            # Unpack MessagePack data
            msgpack_chunk = msgpack.unpackb(raw_data_bytes, raw=False)
            
            # Extract metadata only (not full arrays for performance)
            metadata = {}
            
            for key, value in msgpack_chunk.items():
                if isinstance(value, dict) and value.get('type') in ['ndarray', 'ndarray_compressed']:
                    # For arrays, just extract shape and dtype info
                    metadata[key] = {
                        'shape': value.get('shape', []),
                        'dtype': value.get('dtype', 'unknown'),
                        'compressed': value.get('type') == 'ndarray_compressed'
                    }
                else:
                    # Keep simple values as-is
                    metadata[key] = value
            
            return json.dumps(metadata)
            
        except Exception as e:
            return json.dumps({'error': str(e), 'chunk_id': 'unknown'})
    
    return udf(deserialize_chunk_data, StringType())


def process_streaming_batch(df, epoch_id):
    """
    Process streaming batch para FASE 2: Descomposici√≥n y Agrupaci√≥n BDA - VERSI√ìN CONCISA.
    """
    
    current_time = datetime.now().strftime("%H:%M:%S")
    start_time = time.time()
    
    try:
        # Obtener datos del DataFrame
        row_count = df.count()
        
        if row_count == 0:
            print(f"ÔøΩ [{current_time}] Microbatch {epoch_id}: 0 chunks (waiting...)")
            return
            
        # Recolectar y procesar mensajes de Kafka
        kafka_rows = df.collect()
        total_chunks = 0
        total_rows = 0
        all_groups = {}
        
        print(f"\nüöÄ [{current_time}] Microbatch {epoch_id}: {len(kafka_rows)} chunks")
        print("-" * 50)
        
        # Procesar cada chunk
        for kafka_row in kafka_rows:
            try:
                # Deserializar y descomponer chunk
                raw_bytes = kafka_row.chunk_data
                chunk = deserialize_chunk_data(raw_bytes)
                
                if not chunk:
                    continue
                    
                total_chunks += 1
                rows = decompose_chunk_to_rows(chunk)
                total_rows += len(rows)
                
                # Logging conciso del chunk
                log_chunk_summary(chunk, rows)
                
                # Agrupar filas por baseline+scan
                chunk_groups = group_rows_by_baseline_scan(rows)
                for group_key, group_rows in chunk_groups.items():
                    if group_key not in all_groups:
                        all_groups[group_key] = []
                    all_groups[group_key].extend(group_rows)
                
            except Exception as e:
                print(f"‚ùå Chunk error: {e}")
                continue
        
        # APLICAR BDA A LOS GRUPOS
        if all_groups:
            print("\nüî¨ Applying BDA to groups...")
            
            try:
                # Cargar configuraci√≥n BDA
                config_path = str(project_root / "configs" / "bda_config.json")
                bda_config = load_bda_config_with_fallback(config_path)
                
                # Convertir grupos a lista de filas para BDA
                all_rows = []
                for group_rows in all_groups.values():
                    all_rows.extend(group_rows)
                
                # Aplicar BDA
                bda_results = process_microbatch_with_bda(all_rows, bda_config)
                
                # Crear estad√≠sticas resumen
                bda_stats = create_bda_summary_stats(bda_results)
                
                # Logging de resultados BDA
                print(f"üéØ BDA Results:")
                print(f"   üìä Input rows: {bda_stats.get('total_input_rows', 0)}")
                print(f"   üìà Output windows: {bda_stats.get('total_windows', 0)}")
                print(f"   üìâ Compression: {bda_stats.get('compression_ratio', 0):.2f}:1")
                print(f"   ‚è±Ô∏è  Avg window duration: {bda_stats.get('avg_window_duration_s', 0):.2f}s")
                
                # Formatear algunos resultados para inspecci√≥n
                if bda_results:
                    sample_results = bda_results[:3]  # Primeros 3 para inspecci√≥n
                    print("\nüìã Sample BDA Results:")
                    for i, result in enumerate(sample_results):
                        formatted = format_bda_result_for_output(result)
                        print(f"   Window {i+1}: baseline({result['antenna1']}-{result['antenna2']}) "
                              f"averaged {result['n_input_rows']} rows ‚Üí "
                              f"Œît={result['window_dt_s']:.2f}s")
                
            except Exception as e:
                print(f"‚ùå BDA processing error: {e}")
                # Continuar sin BDA si hay errores
            
        # Resumen final del microbatch
        processing_time = (time.time() - start_time) * 1000
        print("-" * 50)
        print(f"‚úÖ Summary: {total_chunks} chunks | {total_rows} rows | {len(all_groups)} groups | {processing_time:.0f}ms")
        
        if all_groups:
            log_groups_summary(all_groups)
        
    except Exception as e:
        print(f"‚ùå Microbatch {epoch_id} error: {e}")
        
    print()  # L√≠nea en blanco para separar microbatches


def simulate_distributed_processing(chunk_metadata: dict, partition_id: int) -> float:
    """
    Simulate distributed processing work for validation.
    
    This function simulates the computational work that would be done
    in real BDA processing, distributed across different cores.
    
    Parameters
    ----------
    chunk_metadata : dict
        Chunk metadata containing array shapes and properties
    partition_id : int
        Spark partition ID (maps to core)
        
    Returns
    -------
    float
        Simulated processing time in milliseconds
    """
    
    # Simulate work based on data size and partition
    data_points = 1
    for key in ['uvw', 'vis', 'weight']:
        if key in chunk_metadata and 'shape' in chunk_metadata[key]:
            shape = chunk_metadata[key]['shape']
            data_points *= max(shape) if shape else 1
    
    # Simulate processing time (varies by core/partition)
    base_time = min(data_points / 100000, 0.1)  # Scale with data size
    core_variation = (partition_id + 1) * 0.01    # Small variation per core
    
    simulated_time = base_time + core_variation + random.uniform(-0.01, 0.01)
    
    # Actually consume some CPU time for realism
    time.sleep(max(simulated_time, 0.001))
    
    return simulated_time * 1000  # Return in milliseconds


def run_spark_consumer(kafka_servers: str = "localhost:9092", 
                      topic: str = "visibility-stream",
                      config_path: str = None) -> None:
    """
    Run the Spark streaming consumer with enhanced debugging and real-time feedback.
    """
    
    start_time = datetime.now().strftime("%H:%M:%S")
    
    print("üöÄ BDA Interferometry Spark Consumer - FASE 2")
    print("üéØ FEATURES: Row Decomposition + Baseline+Scan Grouping")
    print("=" * 70)
    print(f"üì° Kafka Servers: {kafka_servers}")
    print(f"üìª Topic: {topic}")
    print(f"üïê Started: {start_time}")
    print(f"‚ö° Microbatch Trigger: 10 seconds")
    print(f"üîÑ No Windowing (deferred to Fase 3)")
    print("=" * 70)
    
    # Check Kafka connectivity first
    print("üîç Phase 1: Checking Kafka connectivity...")
    if not wait_for_kafka_ready(kafka_servers, timeout=60):
        print("‚ùå Kafka is not ready. Please check Kafka service status.")
        print("üí° Try: docker-compose up -d")
        return
    print("‚úÖ Kafka is ready and responsive")
    
    # Ensure Kafka topic exists before connecting
    print(f"\nüîç Phase 2: Verifying topic '{topic}'...")
    topic_ready = ensure_kafka_topic_exists(kafka_servers, topic, num_partitions=4, max_retries=30)
    
    if topic_ready:
        print(f"‚úÖ Topic '{topic}' is ready for streaming")
    else:
        print(f"‚ö†Ô∏è  Topic '{topic}' not confirmed, proceeding with auto-create")
    
    # Add a small delay to ensure topic propagation
    print("‚è≥ Allowing time for topic propagation...")
    time.sleep(2)
    
    # Create Spark session
    print("\nÔøΩ Phase 3: Initializing Spark session...")
    spark = create_spark_session(config_path)
    print(f"‚úÖ Spark session created with {spark.sparkContext.defaultParallelism} cores")
    
    try:
        # Create deserializer UDF
        deserialize_udf = deserialize_chunk_udf()
        print("‚úÖ Deserializer UDF configured")
        
        # Read from Kafka with enhanced configuration for robustness
        print("\nÔøΩ Phase 4: Connecting to Kafka stream...")
        kafka_df = spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_servers) \
            .option("subscribe", topic) \
            .option("startingOffsets", "earliest") \
            .option("failOnDataLoss", "false") \
            .option("kafka.auto.create.topics.enable", "true") \
            .option("kafka.num.partitions", "4") \
            .option("kafka.metadata.max.age.ms", "5000") \
            .option("kafka.session.timeout.ms", "30000") \
            .option("kafka.request.timeout.ms", "40000") \
            .option("kafka.retry.backoff.ms", "1000") \
            .option("maxOffsetsPerTrigger", "1000") \
            .load()
        
        print("‚úÖ Connected to Kafka stream")
        
        # Simple processing without windowing (Fase 2)
        processed_df = kafka_df.select(
            kafka_df.key.cast("string").alias("message_key"),
            kafka_df.value.alias("chunk_data"),
            kafka_df.timestamp.alias("kafka_timestamp")
        )
        
        print("‚úÖ Streaming configured for Fase 2 (chunk decomposition + baseline grouping)")
        
        # Configure Spark for optimal performance
        spark.conf.set("spark.sql.shuffle.partitions", "4")
        spark.conf.set("spark.sql.adaptive.enabled", "false")
        
        # Start streaming query with detailed monitoring
        print("\nüîç Phase 5: Starting streaming query...")
        query = processed_df.writeStream \
            .foreachBatch(process_streaming_batch) \
            .outputMode("append") \
            .trigger(processingTime='10 seconds') \
            .option("checkpointLocation", "/tmp/spark-bda-fase2-checkpoint") \
            .start()
        
        print("‚úÖ Consumer ready - waiting for data...")
        print(f"üîÑ Microbatches every 10s | Spark UI: http://localhost:4040 | Ctrl+C to stop")
        print("=" * 60)
        
        # Wait for termination with simplified status updates
        try:
            while query.isActive:
                time.sleep(30)  # Menos heartbeats frecuentes
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"üíì [{current_time}] Listening...")
        except KeyboardInterrupt:
            print("\nüõë Stopping...")
        
        query.awaitTermination()
        
    except KeyboardInterrupt:
        print("\nüõë Stopping consumer...")
        
    except Exception as e:
        print(f"‚ùå Error in streaming: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        print("\nüîí Shutting down Spark session...")
        spark.stop()
        print("‚úÖ Consumer stopped successfully")


def normalize_baseline_key(antenna1: int, antenna2: int, subms_id: str = None) -> str:
    """
    Normaliza la clave de baseline para consistencia entre consumer y BDA.
    
    Garantiza que (1,3) y (3,1) generen la misma clave, y opcionalmente
    incluye subms_id para distinguir diferentes SubMS.
    
    Parameters
    ----------
    antenna1 : int
        Primera antena del baseline
    antenna2 : int
        Segunda antena del baseline  
    subms_id : str, optional
        ID del SubMS para distinguir entre diferentes particiones
        
    Returns
    -------
    str
        Clave normalizada del baseline
    """
    # Normalizar orden: siempre min-max
    ant_min, ant_max = sorted([antenna1, antenna2])
    
    if subms_id:
        return f"{ant_min}-{ant_max}-{subms_id}"
    else:
        return f"{ant_min}-{ant_max}"


def create_group_key(antenna1: int, antenna2: int, scan_number: int, subms_id: str = None) -> str:
    """
    Crea clave de grupo consistente para (baseline, scan_number).
    
    Parameters
    ----------
    antenna1 : int
        Primera antena
    antenna2 : int
        Segunda antena
    scan_number : int
        N√∫mero de scan
    subms_id : str, optional
        ID del SubMS
        
    Returns
    -------
    str
        Clave de grupo normalizada
    """
    baseline_key = normalize_baseline_key(antenna1, antenna2, subms_id)
    return f"{baseline_key}_scan{scan_number}"


def main():
    """Main entry point for Spark consumer service."""
    
    parser = argparse.ArgumentParser(
        description="BDA Interferometry Spark Consumer Service",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python spark_consumer_service.py
  python spark_consumer_service.py --kafka-servers localhost:9092
  python spark_consumer_service.py --topic my-visibility-stream
        """
    )
    
    parser.add_argument(
        "--kafka-servers",
        default="localhost:9092",
        help="Kafka bootstrap servers (default: localhost:9092)"
    )
    
    parser.add_argument(
        "--topic",
        default="visibility-stream", 
        help="Kafka topic name (default: visibility-stream)"
    )
    
    parser.add_argument(
        "--config",
        help="Path to configuration file (optional)"
    )
    
    args = parser.parse_args()
    
    try:
        run_spark_consumer(
            kafka_servers=args.kafka_servers,
            topic=args.topic,
            config_path=args.config
        )
    except Exception as e:
        print(f"‚ùå Fatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
