"""
Spark Consumer Service - Kafka to Spark Streaming Integration

Consumes interferometry visibility data from Kafka using Spark Structured Streaming.
Deserializes chunks and provides basic distributed processing with console output.

This service replaces the traditional Kafka consumer with Spark streaming capabilities
for distributed processing of large-scale interferometry datasets.
"""

import sys
import argparse
from pathlib import Path
import time
from datetime import datetime, timedelta
import msgpack
import numpy as np
import zlib
import json
import random

from pyspark.sql import SparkSession
from pyspark.sql.functions import udf
from pyspark.sql.types import StringType
from kafka.admin import KafkaAdminClient, NewTopic
from kafka.errors import TopicAlreadyExistsError

# Add src directory to path
project_root = Path(__file__).parent.parent
src_path = project_root / "src"
sys.path.append(str(src_path))

from processing.spark_session import create_spark_session
from bda.bda_processor import process_microbatch_with_bda, format_bda_result_for_output, create_bda_summary_stats
from bda.bda_config import load_bda_config_with_fallback, get_default_bda_config


def ensure_kafka_topic_exists(kafka_servers: str, topic: str, num_partitions: int = 4, max_retries: int = 30) -> bool:
    """
    Ensure that a Kafka topic exists, creating it if necessary with robust retry logic.
    
    This function checks if the specified topic exists in Kafka. If it doesn't exist,
    it creates the topic with the specified number of partitions and waits for it to
    be available. This is essential for the streaming architecture to handle topic
    creation before consumers connect.
    
    Parameters
    ----------
    kafka_servers : str
        Kafka bootstrap servers
    topic : str
        Topic name to create/verify
    num_partitions : int
        Number of partitions for the topic (default: 4 for distributed processing)
    max_retries : int
        Maximum number of retries to wait for topic availability (default: 30)
        
    Returns
    -------
    bool
        True if topic exists or was created successfully
    """
    
    try:
        # Create admin client with longer timeout
        admin_client = KafkaAdminClient(
            bootstrap_servers=kafka_servers.split(','),
            client_id='spark_consumer_admin',
            request_timeout_ms=10000,
            connections_max_idle_ms=60000
        )
        
        # Retry loop to ensure topic is available
        for attempt in range(max_retries):
            try:
                # Check if topic exists
                existing_topics = admin_client.list_topics()
                if topic in existing_topics:
                    print(f"‚úÖ Topic '{topic}' is available (attempt {attempt + 1})")
                    return True
                
                # If this is the first attempt, try to create the topic
                if attempt == 0:
                    topic_config = NewTopic(
                        name=topic,
                        num_partitions=num_partitions,
                        replication_factor=1
                    )
                    
                    print(f"üîß Creating topic '{topic}' with {num_partitions} partitions...")
                    admin_client.create_topics([topic_config])
                    print(f"‚úÖ Topic '{topic}' creation initiated")
                
                # Wait before next check
                if attempt < max_retries - 1:
                    print(f"‚è≥ Waiting for topic '{topic}' to be available... (attempt {attempt + 1}/{max_retries})")
                    time.sleep(1)
                    
            except TopicAlreadyExistsError:
                print(f"‚úÖ Topic '{topic}' already exists (concurrent creation)")
                # Still need to verify it's available
                time.sleep(0.5)
                continue
                
            except Exception as e:
                if attempt == 0:
                    print(f"‚ö†Ô∏è  Could not create topic '{topic}': {e}")
                    print(f"üîÑ Will wait for auto-create or manual creation...")
                time.sleep(1)
                continue
        
        print(f"‚ö†Ô∏è  Topic '{topic}' not confirmed available after {max_retries} attempts")
        print(f"üîÑ Proceeding with streaming (relying on Kafka auto-create)")
        return False
        
    except Exception as e:
        print(f"‚ùå Error managing topic '{topic}': {e}")
        print(f"üîÑ Will rely on Kafka auto-create during streaming")
        return False


def wait_for_kafka_ready(kafka_servers: str, timeout: int = 60) -> bool:
    """
    Wait for Kafka to be ready and responsive.
    
    Parameters
    ----------
    kafka_servers : str
        Kafka bootstrap servers
    timeout : int
        Maximum time to wait in seconds
        
    Returns
    -------
    bool
        True if Kafka is ready
    """
    
    print(f"üîç Checking Kafka connectivity to {kafka_servers}...")
    
    try:        
        end_time = datetime.now() + timedelta(seconds=timeout)
        
        while datetime.now() < end_time:
            try:
                admin_client = KafkaAdminClient(
                    bootstrap_servers=kafka_servers.split(','),
                    client_id='kafka_health_check',
                    request_timeout_ms=5000
                )
                
                # Try to list topics as a connectivity test
                topics = admin_client.list_topics()
                print(f"‚úÖ Kafka is ready! Found {len(topics)} topics")
                return True
                
            except Exception as e:
                remaining = int((end_time - datetime.now()).total_seconds())
                if remaining > 0:
                    print(f"‚è≥ Kafka not ready yet... retrying in 2s (timeout: {remaining}s)")
                    time.sleep(2)
                else:
                    break
        
        print(f"‚ùå Kafka not ready after {timeout}s timeout")
        return False
        
    except Exception as e:
        print(f"‚ùå Cannot check Kafka connectivity: {e}")
        return False


def deserialize_chunk_data(raw_data_bytes: bytes) -> dict:
    """
    Deserialize MessagePack chunk data from Kafka - FASE 2 Version.
    
    Deserializes complete chunk with all arrays for row decomposition.
    
    Parameters
    ----------
    raw_data_bytes : bytes
        Raw MessagePack serialized data from Kafka
        
    Returns
    -------
    dict
        Deserialized chunk with numpy arrays reconstructed
    """
    
    try:
        # Deserialize MessagePack
        msgpack_data = msgpack.unpackb(raw_data_bytes, raw=False, strict_map_key=False)
        
        chunk = {}
        for key, value in msgpack_data.items():
            if isinstance(value, dict) and 'type' in value:
                if value['type'] == 'ndarray_compressed':
                    # Decompress and reconstruct array
                    decompressed = zlib.decompress(value['data'])
                    array = np.frombuffer(decompressed, dtype=value['dtype'])
                    chunk[key] = array.reshape(value['shape'])
                elif value['type'] == 'ndarray':
                    # Reconstruct array directly
                    array = np.frombuffer(value['data'], dtype=value['dtype'])
                    chunk[key] = array.reshape(value['shape'])
                else:
                    chunk[key] = value
            else:
                chunk[key] = value
                
        return chunk
        
    except Exception as e:
        print(f"‚ùå Error deserializing chunk: {e}")
        return {}


def decompose_chunk_to_rows(chunk: dict) -> list:
    """
    Descompone un chunk en filas individuales para procesamiento BDA - FASE 2.
    
    Convierte un chunk que contiene m√∫ltiples filas de visibilidades
    en una lista de registros individuales, cada uno representando
    una fila con su baseline_key y scan_number.
    
    Parameters
    ----------
    chunk : dict
        Chunk deserializado con arrays cient√≠ficos
        
    Returns
    -------
    list
        Lista de filas individuales con baseline_key y metadata
    """

    rows = []
    
    try:
        nrows = chunk.get('nrows', 0)
        subms_id = chunk.get('subms_id', 'unknown')
        
        if nrows == 0:
            return rows
            
        # Extraer arrays principales
        antenna1 = chunk.get('antenna1', np.array([]))
        antenna2 = chunk.get('antenna2', np.array([]))
        scan_number = chunk.get('scan_number', np.array([]))
        time_array = chunk.get('time', np.array([]))
        u_array = chunk.get('u', np.array([]))
        v_array = chunk.get('v', np.array([]))
        w_array = chunk.get('w', np.array([]))
        visibilities = chunk.get('visibilities', np.array([]))
        weight = chunk.get('weight', np.array([]))
        flag = chunk.get('flag', np.array([]))
        
        # Validar dimensiones
        if len(antenna1) != nrows or len(antenna2) != nrows:
            print(f"‚ö†Ô∏è  Dimension mismatch in chunk {chunk.get('chunk_id', 'unknown')}")
            return rows
            
        # Descomponer fila por fila
        for row_idx in range(nrows):
            ant1 = int(antenna1[row_idx]) if len(antenna1) > row_idx else -1
            ant2 = int(antenna2[row_idx]) if len(antenna2) > row_idx else -1
            scan_num = int(scan_number[row_idx]) if len(scan_number) > row_idx else -1
            
            # Crear baseline_key normalizado
            baseline_key = normalize_baseline_key(ant1, ant2, subms_id)
            
            # Crear registro de fila individual
            row_record = {
                # Identificadores para agrupaci√≥n
                'baseline_key': baseline_key,
                'scan_number': scan_num,
                'antenna1': ant1,
                'antenna2': ant2,
                'subms_id': subms_id,
                
                # Metadata temporal y espacial
                'time': float(time_array[row_idx]) if len(time_array) > row_idx else 0.0,
                'u': float(u_array[row_idx]) if len(u_array) > row_idx else 0.0,
                'v': float(v_array[row_idx]) if len(v_array) > row_idx else 0.0,
                'w': float(w_array[row_idx]) if len(w_array) > row_idx else 0.0,
                
                # Datos cient√≠ficos reales para BDA
                'visibility': visibilities[row_idx] if len(visibilities) > row_idx else np.array([]),
                'weight': weight[row_idx] if len(weight) > row_idx else np.array([]),
                'flag': flag[row_idx] if len(flag) > row_idx else np.array([]),
                
                # Metadata original del chunk
                'original_chunk_id': chunk.get('chunk_id', -1),
                'row_index_in_chunk': row_idx,
                'field_id': chunk.get('field_id', -1),
                'spw_id': chunk.get('spw_id', -1),
            }
            
            rows.append(row_record)
            
    except Exception as e:
        print(f"‚ùå Error decomposing chunk: {e}")
        
    return rows


def group_rows_by_baseline_scan(rows: list) -> dict:
    """
    Agrupa filas individuales por baseline_key + scan_number - FASE 2.
    
    Parameters
    ----------
    rows : list
        Lista de filas individuales decomposed from chunks
        
    Returns
    -------
    dict
        Diccionario con groups key = (baseline_key, scan_number)
    """
    groups = {}
    
    for row in rows:
        baseline_key = row.get('baseline_key', 'unknown')
        scan_number = row.get('scan_number', -1)
        
        # Crear group key usando funci√≥n consistente
        # Extraer antenna1, antenna2 del baseline_key si es posible
        if 'antenna1' in row and 'antenna2' in row:
            ant1 = row['antenna1']
            ant2 = row['antenna2']
            subms_id = row.get('subms_id', None)
            group_key = create_group_key(ant1, ant2, scan_number, subms_id)
        else:
            # Fallback al m√©todo anterior
            group_key = f"{baseline_key}_scan{scan_number}"
        
        if group_key not in groups:
            groups[group_key] = []
            
        groups[group_key].append(row)
        
    return groups


def log_chunk_summary(chunk_metadata: dict, rows: list) -> None:
    """
    Logging conciso con informaci√≥n esencial del chunk - FASE 2.
    """
    chunk_id = chunk_metadata.get('chunk_id', 'unknown')
    subms_id = chunk_metadata.get('subms_id', 'unknown')
    
    # Calcular estad√≠sticas esenciales
    unique_baselines = set([row.get('baseline_key', 'unknown') for row in rows])
    unique_scans = set([row.get('scan_number', -1) for row in rows])
    times = [row.get('time', 0.0) for row in rows]
    avg_time = sum(times) / len(times) if times else 0.0
    
    print(f"üì¶ Chunk {subms_id}_{chunk_id}: {len(rows)} rows | {len(unique_baselines)} baselines | {len(unique_scans)} scans | time={avg_time:.1f}")


def log_groups_summary(groups: dict) -> None:
    """
    Logging conciso de grupos por baseline + scan - FASE 2.
    """
    total_rows = sum(len(group_rows) for group_rows in groups.values())
    print(f"ÔøΩ Groups: {len(groups)} | Total rows: {total_rows}")


def deserialize_chunk_udf():
    """
    Create UDF for deserializing MessagePack chunks in Spark.
    
    Returns
    -------
    function
        Spark UDF for chunk deserialization
    """
    
    def deserialize_chunk_data(raw_data_bytes):
        """
        Deserialize MessagePack chunk data.
        
        Parameters
        ----------
        raw_data_bytes : bytes
            Raw MessagePack serialized data
            
        Returns
        -------
        str
            JSON string of deserialized chunk metadata
        """
        try:
            # Unpack MessagePack data
            msgpack_chunk = msgpack.unpackb(raw_data_bytes, raw=False)
            
            # Extract metadata only (not full arrays for performance)
            metadata = {}
            
            for key, value in msgpack_chunk.items():
                if isinstance(value, dict) and value.get('type') in ['ndarray', 'ndarray_compressed']:
                    # For arrays, just extract shape and dtype info
                    metadata[key] = {
                        'shape': value.get('shape', []),
                        'dtype': value.get('dtype', 'unknown'),
                        'compressed': value.get('type') == 'ndarray_compressed'
                    }
                else:
                    # Keep simple values as-is
                    metadata[key] = value
            
            return json.dumps(metadata)
            
        except Exception as e:
            return json.dumps({'error': str(e), 'chunk_id': 'unknown'})
    
    return udf(deserialize_chunk_data, StringType())


# Sprint 1: Removed simple processing function that used collect()
# Only distributed processing remains

def process_streaming_batch_distributed(df, epoch_id):
    """
    Process streaming batch with DISTRIBUTED BDA processing (Sprints 2-5).
    
    This function implements the complete distributed processing pipeline:
    - Sprint 2: Distributed deserialization and row expansion
    - Sprint 3: Distributed grouping by (baseline, scan)
    - Sprint 4: Distributed BDA processing on Spark workers
    - Sprint 5: Distributed final aggregation
    
    Parameters
    ----------
    df : DataFrame
        Spark DataFrame with Kafka messages
    epoch_id : int
        Microbatch epoch ID
    """
    from pyspark.sql.functions import count, avg, sum as spark_sum, countDistinct
    from bda.bda_integration import apply_distributed_bda_pipeline
    
    current_time = datetime.now().strftime("%H:%M:%S")
    start_time = time.time()
    
    try:
        # Check if we have data to process
        row_count = df.count()
        
        if row_count == 0:
            print(f"‚è∞ [{current_time}] Microbatch {epoch_id}: 0 chunks (waiting...)")
            return
            
        print(f"\nüöÄ‚ú® [{current_time}] Microbatch {epoch_id}: {row_count} chunks - DISTRIBUTED BDA PROCESSING")
        print("=" * 80)
        
        # Load BDA configuration
        config_path = str(project_root / "configs" / "bda_config.json")
        bda_config = load_bda_config_with_fallback(config_path)
        
        print(f"üîß BDA Config: freq={bda_config['frequency_hz']/1e9:.1f}GHz, "
              f"decorr={bda_config['decorr_factor']}, safety={bda_config['safety_factor']}")
        
        # Apply complete distributed BDA pipeline with Sprint 5 enhancements
        try:
            # Sprint 5: Enhanced pipeline with KPIs and production optimizations
            pipeline_result = apply_distributed_bda_pipeline(
                df, 
                bda_config,
                enable_event_time=True,
                watermark_duration="2 minutes",
                config_file_path="configs/bda_config.json"
            )
            
            # Sprint 5: Extract results and KPIs
            final_results_df = pipeline_result['results']
            kpi_summary_df = pipeline_result['kpis']
            pipeline_config = pipeline_result['config']
            spark_config = pipeline_result['spark_config']
            
            # Collect ONLY Sprint 5 distributed KPIs (not raw data)
            print("üìä Sprint 5: Collecting distributed BDA KPIs...")
            bda_kpis = kpi_summary_df.collect()[0]  # Sprint 5: Enhanced KPIs, still only aggregated stats
            
            # Sprint 5: Enhanced KPI reporting with distributed metrics
            total_input_rows = bda_kpis["total_input_rows"] or 0
            total_windows = bda_kpis["total_windows"] or 0
            compression_factor = bda_kpis["compression_factor"] or 1.0
            compression_ratio_pct = bda_kpis["compression_ratio_pct"] or 0.0
            
            # Enhanced logging with Sprint 5 DISTRIBUTED processing info
            processing_time = (time.time() - start_time) * 1000
            
            print(f"‚úÖ SPRINT 5 DISTRIBUTED BDA PROCESSING SUMMARY:")
            print(f"   üì¶ Input chunks: {row_count}")
            print(f"   üìä Total input rows: {total_input_rows}")
            print(f"   üéØ BDA windows generated: {total_windows}")
            print(f"   ÔøΩ Compression factor: {compression_factor:.1f}:1")
            print(f"   üìä Compression ratio: {compression_ratio_pct:.1f}%")
            print(f"   üîó Unique (baseline,scan) groups processed distributedly")
            print(f"   ‚è±Ô∏è  Window duration p50/p90/p99: {bda_kpis.get('window_dt_p50', 0):.3f}s / {bda_kpis.get('window_dt_p90', 0):.3f}s / {bda_kpis.get('window_dt_p99', 0):.3f}s")
            print(f"   ‚ö° Baseline length avg: {bda_kpis.get('baseline_length_avg', 0):.1f} wavelengths")
            print(f"   üîß Spark config: {spark_config.get('spark.sql.shuffle.partitions', 'default')} partitions")
            print(f"   ‚è±Ô∏è  Processing time: {processing_time:.0f}ms")
            print(f"   üéØ‚ú® SPRINT 5 BDA successfully applied with production optimizations")
            
            # Sprint 1: Sample results collection removed to eliminate collect() calls
            print("\nüìã DISTRIBUTED BDA Processing completed successfully")
            print("   Sample results collection disabled for pure distributed processing")
                      
        except Exception as e:
            print(f"‚ùå DISTRIBUTED BDA processing error: {e}")
            import traceback
            traceback.print_exc()
            
            # Sprint 1: No fallback - distributed processing only
            print("‚ùå Distributed processing failed - no fallback available")
            return
        
        print("=" * 80)
        print(f"‚úÖ Microbatch {epoch_id} completed with DISTRIBUTED processing")
        
    except Exception as e:
        print(f"‚ùå Distributed microbatch {epoch_id} error: {e}")
        import traceback
        traceback.print_exc()
        
    print()  # Separator line


def simulate_distributed_processing(chunk_metadata: dict, partition_id: int) -> float:
    """
    Simulate distributed processing work for validation.
    
    This function simulates the computational work that would be done
    in real BDA processing, distributed across different cores.
    
    Parameters
    ----------
    chunk_metadata : dict
        Chunk metadata containing array shapes and properties
    partition_id : int
        Spark partition ID (maps to core)
        
    Returns
    -------
    float
        Simulated processing time in milliseconds
    """
    
    # Simulate work based on data size and partition
    data_points = 1
    for key in ['uvw', 'vis', 'weight']:
        if key in chunk_metadata and 'shape' in chunk_metadata[key]:
            shape = chunk_metadata[key]['shape']
            data_points *= max(shape) if shape else 1
    
    # Simulate processing time (varies by core/partition)
    base_time = min(data_points / 100000, 0.1)  # Scale with data size
    core_variation = (partition_id + 1) * 0.01    # Small variation per core
    
    simulated_time = base_time + core_variation + random.uniform(-0.01, 0.01)
    
    # Actually consume some CPU time for realism
    time.sleep(max(simulated_time, 0.001))
    
    return simulated_time * 1000  # Return in milliseconds


def run_spark_consumer(kafka_servers: str = "localhost:9092", 
                      topic: str = "visibility-stream",
                      config_path: str = None) -> None:
    """
    Run the Spark streaming consumer with DISTRIBUTED BDA processing.
    
    This consumer uses only distributed processing with Spark UDFs:
    - Sprint 1: Optimized Kafka ‚Üí Spark pipeline
    - Sprint 2: Distributed deserialization UDF  
    - Sprint 3: Distributed grouping and aggregation
    - Watermarks and event time for streaming robustness
    
    Parameters
    ----------
    kafka_servers : str
        Kafka bootstrap servers
    topic : str
        Kafka topic name
    config_path : str, optional
        Path to configuration file
    """
    
    start_time = datetime.now().strftime("%H:%M:%S")
    
    print("üöÄ BDA Interferometry Spark Consumer - DISTRIBUTED PROCESSING")
    print("üéØ FEATURES: Distributed BDA + Event Time + Watermarks")
    print("=" * 70)
    print(f"üì° Kafka Servers: {kafka_servers}")
    print(f"üìª Topic: {topic}")
    print(f"üïê Started: {start_time}")
    print(f"‚ö° Microbatch Trigger: 10 seconds")
    print("‚ú® Pipeline: Kafka ‚Üí Deserialize UDF ‚Üí Group ‚Üí BDA UDF ‚Üí Aggregate")
    print("üåä Watermarks: 2 minutes for late data handling")
    print("=" * 70)
    
    # Check Kafka connectivity first
    print("üîç Phase 1: Checking Kafka connectivity...")
    if not wait_for_kafka_ready(kafka_servers, timeout=60):
        print("‚ùå Kafka is not ready. Please check Kafka service status.")
        print("üí° Try: docker-compose up -d")
        return
    print("‚úÖ Kafka is ready and responsive")
    
    # Ensure Kafka topic exists before connecting
    print(f"\nüîç Phase 2: Verifying topic '{topic}'...")
    topic_ready = ensure_kafka_topic_exists(kafka_servers, topic, num_partitions=4, max_retries=30)
    
    if topic_ready:
        print(f"‚úÖ Topic '{topic}' is ready for streaming")
    else:
        print(f"‚ö†Ô∏è  Topic '{topic}' not confirmed, proceeding with auto-create")
    
    # Add a small delay to ensure topic propagation
    print("‚è≥ Allowing time for topic propagation...")
    time.sleep(2)
    
    # Create Spark session
    print("\nÔøΩ Phase 3: Initializing Spark session...")
    spark = create_spark_session(config_path)
    print(f"‚úÖ Spark session created with {spark.sparkContext.defaultParallelism} cores")
    
    try:
        # Create deserializer UDF
        deserialize_udf = deserialize_chunk_udf()
        print("‚úÖ Deserializer UDF configured")
        
        # Read from Kafka with enhanced configuration for robustness
        print("\nÔøΩ Phase 4: Connecting to Kafka stream...")
        kafka_df = spark \
            .readStream \
            .format("kafka") \
            .option("kafka.bootstrap.servers", kafka_servers) \
            .option("subscribe", topic) \
            .option("startingOffsets", "earliest") \
            .option("failOnDataLoss", "false") \
            .option("kafka.auto.create.topics.enable", "true") \
            .option("kafka.num.partitions", "8") \
            .option("kafka.metadata.max.age.ms", "5000") \
            .option("kafka.session.timeout.ms", "30000") \
            .option("kafka.request.timeout.ms", "40000") \
            .option("kafka.retry.backoff.ms", "1000") \
            .option("maxOffsetsPerTrigger", "5000") \
            .load()
        
        print("‚úÖ Connected to Kafka stream with Sprint 1 optimizations")
        
        # Process with event time and watermarks for robustness
        processed_df = kafka_df.select(
            kafka_df.key.cast("string").alias("message_key"),
            kafka_df.value.alias("chunk_data"),
            kafka_df.timestamp.alias("kafka_timestamp"),
            kafka_df.timestamp.alias("event_time")  # Add event time for watermarks
        )
        
        # Add watermark for late data handling
        watermarked_df = processed_df.withWatermark("event_time", "2 minutes")
        
        print("‚úÖ Watermarks configured: 2 minutes for late data tolerance")
        
        # Configure Spark for optimal distributed performance
        spark.conf.set("spark.sql.shuffle.partitions", "16")  # Increased for better parallelism
        spark.conf.set("spark.sql.adaptive.enabled", "true")   # Enable adaptive query execution
        spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
        spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")
        
        print("‚úÖ Spark optimized for distributed processing")
        
        # Start streaming query with appropriate processing mode
        print("\nüîç Phase 5: Starting streaming query...")
        
        # Use only distributed processing
        print("‚ú® Using DISTRIBUTED BDA processing with watermarks")
        
        query = watermarked_df.writeStream \
            .foreachBatch(process_streaming_batch_distributed) \
            .outputMode("append") \
            .trigger(processingTime='10 seconds') \
            .option("checkpointLocation", "/tmp/spark-bda-distributed-checkpoint") \
            .start()
        
        print("‚úÖ Consumer ready - waiting for data...")
        print(f"üîÑ Microbatches every 10s | Spark UI: http://localhost:4040 | Ctrl+C to stop")
        print("=" * 60)
        
        # Wait for termination with simplified status updates
        try:
            while query.isActive:
                time.sleep(30)  # Menos heartbeats frecuentes
                current_time = datetime.now().strftime("%H:%M:%S")
                print(f"üíì [{current_time}] Listening...")
        except KeyboardInterrupt:
            print("\nüõë Stopping...")
        
        query.awaitTermination()
        
    except KeyboardInterrupt:
        print("\nüõë Stopping consumer...")
        
    except Exception as e:
        print(f"‚ùå Error in streaming: {e}")
        import traceback
        traceback.print_exc()
        
    finally:
        print("\nüîí Shutting down Spark session...")
        spark.stop()
        print("‚úÖ Consumer stopped successfully")


def normalize_baseline_key(antenna1: int, antenna2: int, subms_id: str = None) -> str:
    """
    Normaliza la clave de baseline para consistencia entre consumer y BDA.
    
    Garantiza que (1,3) y (3,1) generen la misma clave, y opcionalmente
    incluye subms_id para distinguir diferentes SubMS.
    
    Parameters
    ----------
    antenna1 : int
        Primera antena del baseline
    antenna2 : int
        Segunda antena del baseline  
    subms_id : str, optional
        ID del SubMS para distinguir entre diferentes particiones
        
    Returns
    -------
    str
        Clave normalizada del baseline
    """
    # Normalizar orden: siempre min-max
    ant_min, ant_max = sorted([antenna1, antenna2])
    
    if subms_id:
        return f"{ant_min}-{ant_max}-{subms_id}"
    else:
        return f"{ant_min}-{ant_max}"


def create_group_key(antenna1: int, antenna2: int, scan_number: int, subms_id: str = None) -> str:
    """
    Crea clave de grupo consistente para (baseline, scan_number).
    
    Parameters
    ----------
    antenna1 : int
        Primera antena
    antenna2 : int
        Segunda antena
    scan_number : int
        N√∫mero de scan
    subms_id : str, optional
        ID del SubMS
        
    Returns
    -------
    str
        Clave de grupo normalizada
    """
    baseline_key = normalize_baseline_key(antenna1, antenna2, subms_id)
    return f"{baseline_key}_scan{scan_number}"


def main():
    """Main entry point for Spark consumer service."""
    
    parser = argparse.ArgumentParser(
        description="BDA Interferometry Spark Consumer Service",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  python consumer_service.py                              # Distributed BDA processing
  python consumer_service.py --kafka-servers localhost:9092
  python consumer_service.py --topic my-visibility-stream
  python consumer_service.py --config /path/to/bda_config.json
        """
    )
    
    parser.add_argument(
        "--kafka-servers",
        default="localhost:9092",
        help="Kafka bootstrap servers (default: localhost:9092)"
    )
    
    parser.add_argument(
        "--topic",
        default="visibility-stream", 
        help="Kafka topic name (default: visibility-stream)"
    )
    
    parser.add_argument(
        "--config",
        help="Path to configuration file (optional)"
    )
    
    args = parser.parse_args()
    
    try:
        run_spark_consumer(
            kafka_servers=args.kafka_servers,
            topic=args.topic,
            config_path=args.config
        )
    except Exception as e:
        print(f"‚ùå Fatal error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
