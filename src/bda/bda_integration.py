"""
BDA Integration - Functional Integration with Consumer Pipeline

Pure integration functions for applying BDA processing to consumer pipeline.
All functions are pure (no side effects) and use functional programming.

This module acts as bridge between consumer service and BDA scientific functions,
maintaining compatibility with existing data structure.
"""

import time
from typing import Dict, List, Any, Tuple
from .bda_processor import (
    process_microbatch_with_bda,
    create_bda_summary_stats,
    format_bda_result_for_output
)
from .bda_core import create_bda_config


def apply_bda_to_groups(all_groups: Dict[str, List[Dict[str, Any]]], 
                       config: Dict[str, float] = None) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Any]]:
    """
    Apply BDA processing to all baseline+scan groups of a microbatch.
    
    This is the main integration function that takes groups generated by
    consumer and applies BDA to each independently. Maintains same
    input and output data structure for complete compatibility.
    
    Parameters
    ----------
    all_groups : Dict[str, List[Dict[str, Any]]]
        Grupos de filas por baseline+scan desde consumer
        Formato: {group_key: [row_dict1, row_dict2, ...]}
    params : BDAParameters, optional
        Par√°metros BDA. Si None, usa valores por defecto
        
    Returns
    -------
    Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Any]]
        (grupos_con_bda, estad√≠sticas_bda)
    """
    if config is None:
        config = create_bda_config()
    
    start_time = time.time()
    
    # Diccionario para almacenar grupos procesados
    bda_groups = {}
    
    # Procesar cada grupo independientemente
    for group_key, group_rows in all_groups.items():
        try:
            # Aplicar BDA al grupo
            # Procesar grupo con BDA - usar nueva API
            averaged_rows = process_microbatch_with_bda(group_rows, config)
            
            # Validaci√≥n b√°sica
            if not averaged_rows:
                print(f"‚ö†Ô∏è  BDA returned empty results for group {group_key}")
            
            # Almacenar resultado
            bda_groups[group_key] = averaged_rows
            
        except Exception as e:
            print(f"‚ùå BDA error in group {group_key}: {e}")
            # En caso de error, mantener filas originales
            bda_groups[group_key] = group_rows.copy()
    
    # Calcular estad√≠sticas completas
    # Crear estad√≠sticas usando nueva API
    all_results = []
    for group_results in bda_groups.values():
        all_results.extend(group_results)
    bda_stats = create_bda_summary_stats(all_results)
    bda_stats['processing_time_ms'] = (time.time() - start_time) * 1000
    
    return bda_groups, bda_stats


def log_bda_results(bda_groups: Dict[str, List[Dict[str, Any]]], 
                   bda_stats: Dict[str, Any]) -> None:
    """
    Genera logging detallado de los resultados de BDA processing.
    
    Imprime estad√≠sticas de compresi√≥n, performance y distribuci√≥n de averaging
    en formato legible para monitoreo en tiempo real.
    
    Parameters
    ----------
    bda_groups : Dict[str, List[Dict[str, Any]]]
        Grupos procesados con BDA
    bda_stats : Dict[str, Any]
        Estad√≠sticas calculadas del procesamiento
    """
    print("\nüéØ BDA PROCESSING RESULTS")
    print("=" * 50)
    
    # Estad√≠sticas principales
    print(f"üìä Groups processed: {bda_stats['groups_processed']}")
    print(f"üì• Input rows: {bda_stats['total_input_rows']}")
    print(f"üì§ Output rows: {bda_stats['total_output_rows']}")
    print(f"üóúÔ∏è  Compression ratio: {bda_stats['compression_ratio']:.1%}")
    print(f"‚ö° Processing time: {bda_stats['processing_time_ms']:.1f}ms")
    print(f"üîÑ Rows with averaging: {bda_stats['averaging_applied_rows']}")
    print(f"üì¶ Single-row groups: {bda_stats['single_row_groups']}")
    
    # Distribuci√≥n de tiempos de averaging
    if bda_stats['averaging_time_distribution']:
        import numpy as np
        avg_times = bda_stats['averaging_time_distribution']
        print(f"\n‚è±Ô∏è  Averaging Times (s):")
        print(f"   Mean: {np.mean(avg_times):.1f}s")
        print(f"   Range: {np.min(avg_times):.1f}s - {np.max(avg_times):.1f}s")
    
    # Estad√≠sticas por baseline (top 5)
    baseline_stats = bda_stats.get('baseline_statistics', {})
    if baseline_stats:
        print(f"\nüì° Top Baselines by Compression:")
        sorted_baselines = sorted(
            baseline_stats.items(), 
            key=lambda x: x[1]['compression_ratio'], 
            reverse=True
        )[:5]
        
        for baseline_key, stats in sorted_baselines:
            print(f"   {baseline_key}: {stats['compression_ratio']:.1%} "
                  f"({stats['input_rows']}‚Üí{stats['output_rows']} rows, "
                  f"{stats['classification']} baseline)")
    
    # Resumen por grupos (sample de 3 grupos)
    print(f"\nüìã Sample Groups:")
    sample_groups = list(bda_groups.items())[:3]
    for group_key, rows in sample_groups:
        averaged_count = sum(1 for r in rows if r.get('bda_applied', False))
        avg_n_averaged = [r.get('bda_n_averaged', 1) for r in rows if r.get('bda_applied', False)]
        avg_reduction = np.mean(avg_n_averaged) if avg_n_averaged else 1.0
        
        print(f"   {group_key}: {len(rows)} rows ({averaged_count} averaged, ~{avg_reduction:.1f}x per avg)")


def create_bda_summary_for_logging(bda_stats: Dict[str, Any]) -> str:
    """
    Crea un resumen conciso de BDA para logging integrado en el consumer.
    
    Parameters
    ----------
    bda_stats : Dict[str, Any]
        Estad√≠sticas BDA
        
    Returns
    -------
    str
        Resumen conciso en una l√≠nea
    """
    compression = bda_stats.get('compression_ratio', 0.0)
    processing_time = bda_stats.get('processing_time_ms', 0.0)
    groups_processed = bda_stats.get('groups_processed', 0)
    input_rows = bda_stats.get('total_input_rows', 0)
    output_rows = bda_stats.get('total_output_rows', 0)
    
    return (f"BDA: {input_rows}‚Üí{output_rows} rows | "
            f"{compression:.1%} compression | "
            f"{groups_processed} groups | "
            f"{processing_time:.0f}ms")


def apply_bda_with_error_handling(all_groups: Dict[str, List[Dict[str, Any]]], 
                                config: Dict[str, float] = None) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Any], bool]:
    """
    Aplica BDA con manejo robusto de errores para entorno de producci√≥n.
    
    Esta funci√≥n wrapper proporciona manejo de errores adicional y fallbacks
    para asegurar que el pipeline nunca falle debido a errores BDA.
    
    Parameters
    ----------
    all_groups : Dict[str, List[Dict[str, Any]]]
        Grupos de filas desde consumer
    params : BDAParameters, optional
        Par√°metros BDA
        
    Returns
    -------
    Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Any], bool]
        (grupos_procesados, estad√≠sticas, √©xito)
    """
    try:
        # Verificar que hay grupos para procesar
        if not all_groups:
            return all_groups, {'error': 'No groups to process'}, False
        
        # Aplicar BDA
        bda_groups, bda_stats = apply_bda_to_groups(all_groups, config)
        
        # Verificaci√≥n final
        total_input = sum(len(rows) for rows in all_groups.values())
        total_output = sum(len(rows) for rows in bda_groups.values())
        
        if total_output == 0 and total_input > 0:
            print("‚ùå BDA processing resulted in zero output rows, using originals")
            return all_groups, {'error': 'Zero output rows'}, False
        
        return bda_groups, bda_stats, True
        
    except Exception as e:
        print(f"‚ùå BDA processing failed: {e}")
        # Fallback: retornar grupos originales
        fallback_stats = {
            'error': str(e),
            'total_input_rows': sum(len(rows) for rows in all_groups.values()),
            'total_output_rows': sum(len(rows) for rows in all_groups.values()),
            'compression_ratio': 0.0,
            'groups_processed': len(all_groups),
            'fallback_used': True
        }
        return all_groups, fallback_stats, False


def get_bda_performance_metrics(bda_stats: Dict[str, Any]) -> Dict[str, float]:
    """
    Extrae m√©tricas de performance clave para monitoreo.
    
    Parameters
    ----------
    bda_stats : Dict[str, Any]
        Estad√≠sticas BDA completas
        
    Returns
    -------
    Dict[str, float]
        M√©tricas de performance clave
    """
    metrics = {
        'compression_ratio': bda_stats.get('compression_ratio', 0.0),
        'processing_time_ms': bda_stats.get('processing_time_ms', 0.0),
        'throughput_rows_per_ms': 0.0,
        'averaging_efficiency': 0.0,
        'groups_processed': float(bda_stats.get('groups_processed', 0)),
    }
    
    # Calcular throughput
    if metrics['processing_time_ms'] > 0:
        total_rows = bda_stats.get('total_input_rows', 0)
        metrics['throughput_rows_per_ms'] = total_rows / metrics['processing_time_ms']
    
    # Calcular efficiency del averaging
    averaged_rows = bda_stats.get('averaging_applied_rows', 0)
    total_output = bda_stats.get('total_output_rows', 0)
    if total_output > 0:
        metrics['averaging_efficiency'] = averaged_rows / total_output
    
    return metrics


def configure_bda_for_observation(frequency_hz: float = None,
                                declination_deg: float = None) -> Dict[str, float]:
    """
    Configura par√°metros BDA optimizados para una observaci√≥n espec√≠fica.
    
    Parameters
    ----------
    frequency_hz : float, optional
        Frecuencia de observaci√≥n en Hz
    declination_deg : float, optional  
        Declinaci√≥n de la fuente en grados
        
    Returns
    -------
    Dict[str, float]
        Configuraci√≥n BDA optimizada
    """
    config = create_bda_config()
    
    if frequency_hz is not None:
        config['frequency_hz'] = frequency_hz
    
    if declination_deg is not None:
        config['declination_deg'] = declination_deg
    
    return config
