"""
BDA Integration - Functional Integration with Consumer Pipeline

Pure integration functions for applying BDA processing to consumer pipeline.
All functions are pure (no side effects) and use functional programming.

This module acts as bridge between consumer service and BDA scientific functions,
maintaining compatibility with existing data structure.
"""

import time
import json
import logging
import math
import traceback
import numpy as np
import pandas as pd
import msgpack
import zlib
from typing import Dict, List, Any, Tuple

# Spark imports
from pyspark.sql.functions import (
    explode, col, lit, count, sum as spark_sum, avg, max as spark_max, 
    pandas_udf, udf
)
from pyspark.sql.types import (
    StructType, StructField, StringType, IntegerType, FloatType, 
    DoubleType, ArrayType, BinaryType, MapType
)

# Local imports
from .bda_processor import (
    process_microbatch_with_bda,
    create_bda_summary_stats,
    format_bda_result_for_output
)
from .bda_core import create_bda_config


def apply_bda_to_groups(all_groups: Dict[str, List[Dict[str, Any]]], 
                       config: Dict[str, float] = None) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Any]]:
    """
    Apply BDA processing to all baseline+scan groups of a microbatch.
    
    Takes groups generated by consumer and applies BDA to each independently while
    maintaining the same input and output data structure for complete compatibility.
    
    Parameters
    ----------
    all_groups : Dict[str, List[Dict[str, Any]]]
        Groups of rows by baseline+scan from consumer
    config : Dict[str, float], optional
        BDA configuration. If None, uses default values
        
    Returns
    -------
    Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Any]]
        Tuple containing processed groups and BDA statistics
    """
    if config is None:
        config = create_bda_config()
    
    start_time = time.time()
    
    # Dictionary to store processed groups
    bda_groups = {}
    
    # Process each group independently
    for group_key, group_rows in all_groups.items():
        try:
            # Apply BDA to group using new API
            averaged_rows = process_microbatch_with_bda(group_rows, config)
            
            # Basic validation
            if not averaged_rows:
                logging.warning(f"BDA returned empty results for group {group_key}")
            
            # Store result
            bda_groups[group_key] = averaged_rows
            
        except Exception as e:
            logging.error(f"BDA error in group {group_key}: {e}")
            # In case of error, keep original rows
            bda_groups[group_key] = group_rows.copy()
    
    # Calculate complete statistics using new API
    all_results = []
    for group_results in bda_groups.values():
        all_results.extend(group_results)
    bda_stats = create_bda_summary_stats(all_results)
    bda_stats['processing_time_ms'] = (time.time() - start_time) * 1000
    
    return bda_groups, bda_stats


def log_bda_results(bda_groups: Dict[str, List[Dict[str, Any]]], 
                   bda_stats: Dict[str, Any]) -> None:
    """
    Generate detailed logging of BDA processing results.
    
    Outputs compression statistics, performance metrics and averaging distribution
    in readable format for real-time monitoring.
    
    Parameters
    ----------
    bda_groups : Dict[str, List[Dict[str, Any]]]
        Groups processed with BDA
    bda_stats : Dict[str, Any]
        Processing statistics from BDA execution
    """
    logger = logging.getLogger(__name__)
    
    logger.info("BDA PROCESSING RESULTS")
    logger.info("=" * 50)
    
    # Main statistics
    logger.info(f"Groups processed: {bda_stats['groups_processed']}")
    logger.info(f"Input rows: {bda_stats['total_input_rows']}")
    logger.info(f"Output rows: {bda_stats['total_output_rows']}")
    logger.info(f"Compression ratio: {bda_stats['compression_ratio']:.1%}")
    logger.info(f"Processing time: {bda_stats['processing_time_ms']:.1f}ms")
    logger.info(f"Rows with averaging: {bda_stats['averaging_applied_rows']}")
    logger.info(f"Single-row groups: {bda_stats['single_row_groups']}")
    
    # Averaging time distribution
    if bda_stats['averaging_time_distribution']:
        avg_times = bda_stats['averaging_time_distribution']
        logger.info(f"Averaging Times (s):")
        logger.info(f"   Mean: {np.mean(avg_times):.1f}s")
        logger.info(f"   Range: {np.min(avg_times):.1f}s - {np.max(avg_times):.1f}s")
    
    # Statistics per baseline (top 5)
    baseline_stats = bda_stats.get('baseline_statistics', {})
    if baseline_stats:
        logger.info(f"Top Baselines by Compression:")
        sorted_baselines = sorted(
            baseline_stats.items(), 
            key=lambda x: x[1]['compression_ratio'], 
            reverse=True
        )[:5]
        
        for baseline_key, stats in sorted_baselines:
            logger.info(f"   {baseline_key}: {stats['compression_ratio']:.1%} "
                      f"({stats['input_rows']}→{stats['output_rows']} rows, "
                      f"{stats['classification']} baseline)")
    
    # Summary per groups (sample of 3 groups)
    logger.info(f"Sample Groups:")
    sample_groups = list(bda_groups.items())[:3]
    for group_key, rows in sample_groups:
        averaged_count = sum(1 for r in rows if r.get('bda_applied', False))
        avg_n_averaged = [r.get('bda_n_averaged', 1) for r in rows if r.get('bda_applied', False)]
        avg_reduction = np.mean(avg_n_averaged) if avg_n_averaged else 1.0
        
        logger.info(f"   {group_key}: {len(rows)} rows ({averaged_count} averaged, ~{avg_reduction:.1f}x per avg)")


def create_bda_summary_for_logging(bda_stats: Dict[str, Any]) -> str:
    """
    Create concise BDA summary for integrated logging in consumer.
    
    Parameters
    ----------
    bda_stats : Dict[str, Any]
        BDA processing statistics
        
    Returns
    -------
    str
        One-line summary string containing key metrics
    """
    compression = bda_stats.get('compression_ratio', 0.0)
    processing_time = bda_stats.get('processing_time_ms', 0.0)
    groups_processed = bda_stats.get('groups_processed', 0)
    input_rows = bda_stats.get('total_input_rows', 0)
    output_rows = bda_stats.get('total_output_rows', 0)
    
    return (f"BDA: {input_rows}→{output_rows} rows | "
            f"{compression:.1%} compression | "
            f"{groups_processed} groups | "
            f"{processing_time:.0f}ms")


def apply_bda_with_error_handling(all_groups: Dict[str, List[Dict[str, Any]]], 
                                config: Dict[str, float] = None) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Any], bool]:
    """
    Apply BDA with robust error handling for production environment.
    
    Provides additional error handling and fallbacks to ensure the pipeline
    never fails due to BDA processing errors.
    
    Parameters
    ----------
    all_groups : Dict[str, List[Dict[str, Any]]]
        Groups of rows from consumer
    config : Dict[str, float], optional
        BDA configuration parameters
        
    Returns
    -------
    Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, Any], bool]
        Tuple containing processed groups, statistics, and success flag
    """
    try:
        # Verify there are groups to process
        if not all_groups:
            return all_groups, {'error': 'No groups to process'}, False
        
        # Apply BDA
        bda_groups, bda_stats = apply_bda_to_groups(all_groups, config)
        
        # Final verification
        total_input = sum(len(rows) for rows in all_groups.values())
        total_output = sum(len(rows) for rows in bda_groups.values())
        
        if total_output == 0 and total_input > 0:
            logging.error("BDA processing resulted in zero output rows, using originals")
            return all_groups, {'error': 'Zero output rows'}, False
        
        return bda_groups, bda_stats, True
        
    except Exception as e:
        logging.error(f"BDA processing failed: {e}")
        # Fallback: return original groups
        fallback_stats = {
            'error': str(e),
            'total_input_rows': sum(len(rows) for rows in all_groups.values()),
            'total_output_rows': sum(len(rows) for rows in all_groups.values()),
            'compression_ratio': 0.0,
            'groups_processed': len(all_groups),
            'fallback_used': True
        }
        return all_groups, fallback_stats, False


def get_bda_performance_metrics(bda_stats: Dict[str, Any]) -> Dict[str, float]:
    """
    Extract key performance metrics for monitoring.
    
    Parameters
    ----------
    bda_stats : Dict[str, Any]
        Complete BDA processing statistics
        
    Returns
    -------
    Dict[str, float]
        Dictionary containing key performance metrics
    """
    metrics = {
        'compression_ratio': bda_stats.get('compression_ratio', 0.0),
        'processing_time_ms': bda_stats.get('processing_time_ms', 0.0),
        'throughput_rows_per_ms': 0.0,
        'averaging_efficiency': 0.0,
        'groups_processed': float(bda_stats.get('groups_processed', 0)),
    }
    
    # Calculate throughput
    if metrics['processing_time_ms'] > 0:
        total_rows = bda_stats.get('total_input_rows', 0)
        metrics['throughput_rows_per_ms'] = total_rows / metrics['processing_time_ms']
    
    # Calculate averaging efficiency
    averaged_rows = bda_stats.get('averaging_applied_rows', 0)
    total_output = bda_stats.get('total_output_rows', 0)
    if total_output > 0:
        metrics['averaging_efficiency'] = averaged_rows / total_output
    
    return metrics


def configure_bda_for_observation(frequency_hz: float = None,
                                declination_deg: float = None) -> Dict[str, float]:
    """
    Configure BDA parameters optimized for a specific observation.
    
    Parameters
    ----------
    frequency_hz : float, optional
        Observation frequency in Hz
    declination_deg : float, optional  
        Source declination in degrees
        
    Returns
    -------
    Dict[str, float]
        Optimized BDA configuration dictionary
    """
    config = create_bda_config()
    
    if frequency_hz is not None:
        config['frequency_hz'] = frequency_hz
    
    if declination_deg is not None:
        config['declination_deg'] = declination_deg
    
    return config


def apply_distributed_bda_pipeline(df, bda_config, enable_event_time=False, 
                                  watermark_duration="2 minutes", config_file_path=None):
    """
    Apply fully distributed BDA pipeline with complete scientific processing.
    
    Implements complete distributed BDA pipeline including MessagePack deserialization
    with scientific arrays, distributed grouping by baseline+scan, BDA processing
    using Pandas UDFs, and final distributed aggregations.
    
    Parameters
    ----------
    df : DataFrame
        Spark DataFrame with Kafka data containing MessagePack column 'value'
    bda_config : dict
        Complete BDA configuration parameters
    enable_event_time : bool
        Whether to use event time for watermarking
    watermark_duration : str
        Watermark duration for late data handling
    config_file_path : str
        Path to config file (currently ignored)
        
    Returns
    -------
    dict
        Dictionary with complete distributed BDA results containing DataFrames and configurations
    """
    logger = logging.getLogger(__name__)
    
    logger.info("Starting fully distributed BDA pipeline")
    logger.info("Scientific deserialization with complex arrays")
    logger.info("Distributed BDA UDFs per group") 
    logger.info("Native Spark pipeline (groupBy → applyInPandas)")
    logger.info("Final distributed aggregations")
    
    try:
        # Scientific deserialization
        logger.info("Applying scientific deserialization with real schemas...")
        
        # Apply advanced scientific deserialization
        scientific_deserializer_udf = create_scientific_deserializer_udf()
        
        scientific_df = (df
                        .withColumn("scientific_rows", scientific_deserializer_udf(col("chunk_data")))
                        .select(explode(col("scientific_rows")).alias("scientific_row"))
                        .select("scientific_row.*"))
        
        logger.info("Scientific rows with complex arrays expanded")
        
        # Watermarking for streaming
        if enable_event_time:
            logger.info(f"Applying watermark for late data: {watermark_duration}")
            scientific_df = scientific_df.withWatermark("time", watermark_duration)
        else:
            logger.info("Watermarking disabled")
        
        # Distributed grouping
        logger.info("Distributed grouping by (baseline, scan)...")
        
        # Group by baseline_key and scan_number for distributed BDA
        grouped_df = scientific_df.groupBy("baseline_key", "scan_number")
        
        logger.info("Grouping by baseline+scan configured")
        
        # Distributed BDA per group
        logger.info("Applying distributed BDA with Pandas UDFs...")
        
        # Create distributed BDA UDF
        bda_group_udf = create_distributed_bda_group_processor(bda_config)
        
        # Apply distributed BDA to each group
        bda_results_df = grouped_df.apply(bda_group_udf)
        
        logger.info("Distributed BDA applied to all groups")
        
        # Final distributed aggregations
        logger.info("Calculating final distributed KPIs...")
        
        # Calculate distributed statistics from BDA results
        final_kpis = (bda_results_df
                     .agg(
                         count("*").alias("total_bda_windows"),
                         spark_sum(col("n_input_rows").cast("integer")).alias("total_input_rows"),
                         avg(col("compression_ratio").cast("double")).alias("avg_compression_ratio"),
                         spark_max(col("compression_ratio").cast("double")).alias("max_compression_ratio"),
                         count("group_key").alias("unique_groups_processed"),
                         avg(col("window_duration_s").cast("double")).alias("avg_window_duration_s"),
                         spark_max("processing_timestamp").alias("last_processing_time")
                     )
                     .collect()[0])
        
        # Extract final metrics
        total_windows = int(final_kpis["total_bda_windows"] or 0)
        total_input_rows = int(final_kpis["total_input_rows"] or 0)
        avg_compression_ratio = float(final_kpis["avg_compression_ratio"] or 1.0)
        max_compression_ratio = float(final_kpis["max_compression_ratio"] or 1.0)
        unique_groups = int(final_kpis["unique_groups_processed"] or 0)
        avg_window_duration = float(final_kpis["avg_window_duration_s"] or 0.0)
        
        # Generate final results
        logger.info("Generating final distributed BDA results...")
        
        # Create results without expensive collect() operations
        results_data = [{
            'result_type': 'fully_distributed_bda',
            'processing_status': 'bda_completed_distributed',
            'total_bda_windows': total_windows,
            'total_input_rows': total_input_rows,
            'unique_groups_processed': unique_groups,
            'sample_baseline_keys': min(unique_groups, 10),  # Estimation without collect()
            'sample_avg_compression': avg_compression_ratio,  # Use already calculated value
            'processing_mode': 'distributed_pandas_udf_bda'
        }]
        
        spark_session = df.sparkSession
        results_df = spark_session.createDataFrame(results_data, get_final_bda_results_schema())
        
        # Final distributed KPIs
        kpis_data = [{
            'total_input_rows': int(total_input_rows),
            'total_windows': int(total_windows),
            'compression_factor': float(avg_compression_ratio),
            'compression_ratio_pct': float((1 - 1/avg_compression_ratio) * 100) if avg_compression_ratio > 1 else 0.0
        }]
        
        kpis_df = spark_session.createDataFrame(kpis_data, get_kpis_schema())
        
        # Final result
        logger.info(f"DISTRIBUTED BDA PIPELINE COMPLETED")
        logger.info(f"   Input rows: {total_input_rows}")
        logger.info(f"   BDA windows: {total_windows}")
        logger.info(f"   Average compression: {avg_compression_ratio:.2f}:1")
        logger.info(f"   Maximum compression: {max_compression_ratio:.2f}:1")
        logger.info(f"   Groups processed: {unique_groups}")
        logger.info(f"   Average window duration: {avg_window_duration:.2f}s")
        logger.info(f"   Mode: FULLY DISTRIBUTED")
        
        return {
            'results': bda_results_df,  # Complete DataFrame with BDA results
            'kpis': kpis_df,
            'config': bda_config,
            'spark_config': {
                'mode': 'fully_distributed_bda',
                'total_input_rows': int(total_input_rows),
                'total_bda_windows': int(total_windows),
                'unique_groups_processed': int(unique_groups),
                'avg_compression_ratio': float(avg_compression_ratio),
                'max_compression_ratio': float(max_compression_ratio),
                'pipeline_status': 'complete_distributed_bda_pipeline',
                'architecture': 'spark_pandas_udf_groupby_applyinpandas'
            }
        }
        
    except Exception as e:
        logger.error(f"ERROR IN DISTRIBUTED PIPELINE")
        logger.error(f"   Error: {e}")
        logger.error(traceback.format_exc())
        
        # Safe fallback
        spark_session = df.sparkSession
        empty_results_df = spark_session.createDataFrame([], get_empty_results_schema())
        empty_kpis_df = spark_session.createDataFrame([{
            'total_input_rows': int(0),
            'total_windows': int(0), 
            'compression_factor': float(1.0),
            'compression_ratio_pct': float(0.0)
        }], get_kpis_schema())
        
        return {
            'results': empty_results_df,
            'kpis': empty_kpis_df,
            'config': bda_config,
            'spark_config': {'mode': 'error_fallback', 'error': str(e)}
        }


def get_temporal_results_schema():
    """
    Get temporal schema for BDA results.
    
    Returns
    -------
    StructType
        Schema for temporal BDA results
    """
    
    return StructType([
        StructField("result_type", StringType(), False),
        StructField("processing_status", StringType(), False), 
        StructField("total_windows_generated", IntegerType(), False),
        StructField("compression_achieved", FloatType(), False)
    ])


def get_kpis_schema():
    """
    Get schema for BDA KPIs.
    
    Returns
    -------
    StructType
        Schema for BDA key performance indicators
    """
    
    return StructType([
        StructField("total_input_rows", IntegerType(), True),
        StructField("total_windows", IntegerType(), True),
        StructField("compression_factor", FloatType(), True), 
        StructField("compression_ratio_pct", FloatType(), True)
    ])


def get_empty_results_schema():
    """
    Get empty schema for results when there is no data.
    
    Returns
    -------
    StructType
        Empty schema for fallback results
    """
    
    return StructType([
        StructField("empty_result", StringType(), True)
    ])


def get_empty_kpis_schema():
    """
    Get empty schema for KPIs when there is no data.
    
    Returns
    -------
    StructType
        Empty schema for KPIs fallback
    """
    return get_kpis_schema()


def get_expanded_results_schema():
    """
    Get schema for expanded scientific data results.
    
    Returns
    -------
    StructType
        Schema for expanded scientific visibility data
    """
    
    return StructType([
        StructField("result_type", StringType(), False),
        StructField("processing_status", StringType(), False),
        StructField("total_scientific_rows", IntegerType(), False),
        StructField("sample_baseline_keys", IntegerType(), False),
        StructField("sample_scans", IntegerType(), False),
        StructField("deserialization_mode", StringType(), False)
    ])


def get_scientific_visibility_schema():
    """
    Get Spark schema for scientific visibility data with real arrays.
    
    Defines complete structure for expanded visibility rows with support
    for multidimensional complex arrays, optimized for distributed BDA.
    
    Returns
    -------
    StructType
        Complete schema for scientific visibilities
    """
    
    return StructType([
        # === GROUPING IDENTIFIERS ===
        StructField("baseline_key", StringType(), False),
        StructField("group_key", StringType(), False),  # baseline_key + scan
        StructField("scan_number", IntegerType(), False),
        StructField("antenna1", IntegerType(), False),
        StructField("antenna2", IntegerType(), False),
        StructField("subms_id", StringType(), True),
        
        # === SPATIOTEMPORAL COORDINATES ===
        StructField("time", DoubleType(), False),        # MJD timestamp
        StructField("u", DoubleType(), False),           # U coordinate (wavelengths)
        StructField("v", DoubleType(), False),           # V coordinate (wavelengths)  
        StructField("w", DoubleType(), False),           # W coordinate (wavelengths)
        StructField("baseline_length", DoubleType(), True), # sqrt(u²+v²) calculada
        
        # === COMPLEX SCIENTIFIC ARRAYS ===
        # Visibility: complex array [nchans, npols] 
        StructField("visibility_real", ArrayType(ArrayType(DoubleType())), False),
        StructField("visibility_imag", ArrayType(ArrayType(DoubleType())), False),
        StructField("visibility_shape", ArrayType(IntegerType()), False), # [nchans, npols]
        
        # Weight: real array [nchans, npols]
        StructField("weight", ArrayType(ArrayType(DoubleType())), False),
        StructField("weight_shape", ArrayType(IntegerType()), False),
        
        # Flag: boolean array [nchans, npols] 
        StructField("flag", ArrayType(ArrayType(IntegerType())), False), # 0/1 for boolean
        StructField("flag_shape", ArrayType(IntegerType()), False),
        
        # === METADATA ORIGINAL ===
        StructField("original_chunk_id", IntegerType(), True),
        StructField("row_index_in_chunk", IntegerType(), False),
        StructField("field_id", IntegerType(), True),
        StructField("spw_id", IntegerType(), True),
        
        # === METADATA BDA ===
        StructField("bda_applied", IntegerType(), True),      # 0/1 boolean  
        StructField("bda_n_averaged", IntegerType(), True),   # number of averaged rows
        StructField("bda_time_range", DoubleType(), True),    # rango temporal de averaging
    ])


def get_bda_group_input_schema():
    """
    Get schema for visibility groups going to BDA processing.
    
    Optimized structure for Pandas UDF that processes groups by baseline and scan.
    
    Returns
    -------
    StructType  
        Schema for BDA input per group
    """
    
    return StructType([
        StructField("group_key", StringType(), False),
        StructField("baseline_key", StringType(), False), 
        StructField("scan_number", IntegerType(), False),
        StructField("antenna1", IntegerType(), False),
        StructField("antenna2", IntegerType(), False),
        StructField("row_count", IntegerType(), False),
        
        # Array of all group rows for BDA processing
        StructField("visibility_rows", ArrayType(get_scientific_visibility_schema()), False),
    ])


def get_bda_result_schema():
    """
    Get schema for distributed BDA processing results.
    
    Structure returned by each BDA UDF after averaging operations.
    
    Returns
    -------
    StructType
        Schema for individual BDA results
    """
    
    return StructType([
        # === IDENTIFIERS ===
        StructField("group_key", StringType(), False),
        StructField("baseline_key", StringType(), False),
        StructField("scan_number", IntegerType(), False), 
        StructField("antenna1", IntegerType(), False),
        StructField("antenna2", IntegerType(), False),
        
        # === AVERAGED BDA DATA ===
        StructField("time_avg", DoubleType(), False),
        StructField("u_avg", DoubleType(), False),
        StructField("v_avg", DoubleType(), False),
        StructField("w_avg", DoubleType(), False),
        StructField("baseline_length", DoubleType(), False),
        
        # === AVERAGED VISIBILITY ===
        StructField("visibility_avg_real", ArrayType(ArrayType(DoubleType())), False),
        StructField("visibility_avg_imag", ArrayType(ArrayType(DoubleType())), False),
        StructField("weight_total", ArrayType(ArrayType(DoubleType())), False),
        StructField("flag_combined", ArrayType(ArrayType(IntegerType())), False),
        
        # === BDA STATISTICS ===
        StructField("n_input_rows", IntegerType(), False),
        StructField("n_windows_created", IntegerType(), False),
        StructField("window_duration_s", DoubleType(), False),
        StructField("max_averaging_time_s", DoubleType(), False),
        StructField("compression_ratio", DoubleType(), False),
        
        # === METADATA ===
        StructField("processing_timestamp", DoubleType(), False),
        StructField("bda_config_hash", StringType(), True),
    ])


def create_scientific_deserializer_udf():
    """
    Create UDF for deserialization with scientific schemas.
    
    Handles complex multidimensional arrays using the appropriate
    scientific schema for distributed processing.
    
    Returns
    -------
    function
        UDF that deserializes to complete scientific schema
    """
    
    def deserialize_to_scientific_rows(raw_data_bytes):
        """
        Deserialize MessagePack to scientific rows with real arrays.
        
        Parameters
        ----------
        raw_data_bytes : bytes
            MessagePack chunk data
            
        Returns
        -------
        list
            List of rows with complete scientific schema
        """
        try:
            if raw_data_bytes is None:
                return []
            
            # Deserialize complete chunk
            chunk = deserialize_msgpack_complete(raw_data_bytes)
            
            if not chunk or 'error' in chunk:
                return []
            
            # Expand with complete scientific schema
            return expand_to_scientific_rows(chunk)
                
        except Exception as e:
            logging.error(f"Error in scientific deserializer: {e}")
            return []
    
    return udf(deserialize_to_scientific_rows, ArrayType(get_scientific_visibility_schema()))


def expand_to_scientific_rows(chunk: dict) -> list:
    """
    Expand chunk to scientific rows with multidimensional real arrays.
    
    Handles complex arrays [nchans, npols] correctly and separates them
    into real/imaginary components for Spark compatibility.
    
    Parameters
    ----------
    chunk : dict
        Deserialized chunk with numpy arrays
        
    Returns
    -------
    list
        List of rows with complete scientific schema
    """
    
    rows = []
    
    try:
        nrows = chunk.get('nrows', 0)
        if nrows == 0:
            return rows
        
        # Extract arrays
        antenna1 = chunk.get('antenna1', np.array([]))
        antenna2 = chunk.get('antenna2', np.array([]))
        scan_number = chunk.get('scan_number', np.array([]))
        time_array = chunk.get('time', np.array([]))
        u_array = chunk.get('u', np.array([]))
        v_array = chunk.get('v', np.array([]))
        w_array = chunk.get('w', np.array([]))
        visibilities = chunk.get('visibilities', np.array([]))
        weight = chunk.get('weight', np.array([]))
        flag = chunk.get('flag', np.array([]))
        
        # Validate dimensions
        if len(antenna1) != nrows or len(antenna2) != nrows:
            return rows
        
        # Process row by row
        for row_idx in range(nrows):
            try:
                # Basic identifiers
                ant1 = int(antenna1[row_idx])
                ant2 = int(antenna2[row_idx])
                scan_num = int(scan_number[row_idx])
                
                # Spatiotemporal coordinates
                time_val = float(time_array[row_idx])
                u_val = float(u_array[row_idx])
                v_val = float(v_array[row_idx])
                w_val = float(w_array[row_idx])
                
                # Calculate baseline length
                baseline_length = math.sqrt(u_val**2 + v_val**2)
                
                # Normalized keys
                baseline_key = create_normalized_baseline_key(ant1, ant2)
                group_key = f"{baseline_key}_scan{scan_num}"
                
                # Extract scientific arrays
                vis_array = visibilities[row_idx] if len(visibilities) > row_idx else np.array([[]])
                weight_array = weight[row_idx] if len(weight) > row_idx else np.array([[]])
                flag_array = flag[row_idx] if len(flag) > row_idx else np.array([[]])
                
                # Convert complex arrays to separated real/imag
                vis_real, vis_imag, vis_shape = convert_complex_array_to_spark(vis_array)
                weight_2d, weight_shape = convert_real_array_to_spark(weight_array)
                flag_2d, flag_shape = convert_flag_array_to_spark(flag_array)
                
                # Create complete scientific row
                scientific_row = {
                    # Identifiers
                    'baseline_key': baseline_key,
                    'group_key': group_key,
                    'scan_number': scan_num,
                    'antenna1': ant1,
                    'antenna2': ant2,
                    'subms_id': str(chunk.get('subms_id', 'unknown')),
                    
                    # Coordinates
                    'time': time_val,
                    'u': u_val,
                    'v': v_val,
                    'w': w_val,
                    'baseline_length': baseline_length,
                    
                    # Scientific arrays
                    'visibility_real': vis_real,
                    'visibility_imag': vis_imag,
                    'visibility_shape': vis_shape,
                    'weight': weight_2d,
                    'weight_shape': weight_shape,
                    'flag': flag_2d,
                    'flag_shape': flag_shape,
                    
                    # Metadata
                    'original_chunk_id': int(chunk.get('chunk_id', -1)),
                    'row_index_in_chunk': row_idx,
                    'field_id': int(chunk.get('field_id', -1)),
                    'spw_id': int(chunk.get('spw_id', -1)),
                    
                    # BDA metadata (initial)
                    'bda_applied': 0,
                    'bda_n_averaged': 1,
                    'bda_time_range': 0.0,
                }
                
                rows.append(scientific_row)
                
            except Exception as e:
                # Skip problematic row
                continue
                
    except Exception as e:
        logging.error(f"Error expanding to scientific rows: {e}")
        
    return rows


def convert_complex_array_to_spark(complex_array):
    """
    Convert complex numpy array to Spark format (separated real/imag).
    
    Parameters
    ----------
    complex_array : np.ndarray
        Complex numpy array [nchans, npols] or similar
        
    Returns
    -------
    tuple
        Tuple containing real 2D list, imaginary 2D list, and shape list
    """
    try:
        if complex_array.size == 0:
            return [[]], [[]], [0, 0]
        
        if complex_array.ndim == 1:
            complex_array = complex_array.reshape(-1, 1)
        
        # Split into real and imaginary parts
        real_part = complex_array.real.tolist()
        imag_part = complex_array.imag.tolist()
        shape = list(complex_array.shape)
        
        return real_part, imag_part, shape
        
    except:
        return [[]], [[]], [0, 0]


def convert_real_array_to_spark(real_array):
    """
    Convert real numpy array to Spark 2D format.
    
    Parameters
    ----------
    real_array : np.ndarray
        Real numpy array
        
    Returns
    -------
    tuple
        Tuple containing 2D array list and shape list
    """
    try:
        if real_array.size == 0:
            return [[]], [0, 0]
        
        # Ensure 2D
        if real_array.ndim == 1:
            real_array = real_array.reshape(-1, 1)
            
        return real_array.tolist(), list(real_array.shape)
        
    except:
        return [[]], [0, 0]


def convert_flag_array_to_spark(flag_array):
    """
    Convert boolean flags array to integers for Spark.
    
    Parameters
    ----------
    flag_array : np.ndarray
        Boolean flags array
        
    Returns
    -------
    tuple
        Tuple containing 2D integer flags list and shape list
    """
    try:
        if flag_array.size == 0:
            return [[]], [0, 0]
        
        # Ensure 2D and convert to int
        if flag_array.ndim == 1:
            flag_array = flag_array.reshape(-1, 1)
        
        # Convert boolean to int (0/1)
        flag_int = flag_array.astype(int).tolist()
        shape = list(flag_array.shape)
        
        return flag_int, shape
        
    except:
        return [[]], [0, 0]


def get_final_bda_results_schema():
    """
    Get schema for final results of distributed BDA pipeline.
    
    Returns
    -------
    StructType
        Schema for final distributed BDA results
    """
    
    return StructType([
        StructField("result_type", StringType(), False),
        StructField("processing_status", StringType(), False),
        StructField("total_bda_windows", IntegerType(), False),
        StructField("total_input_rows", IntegerType(), False),
        StructField("unique_groups_processed", IntegerType(), False),
        StructField("sample_baseline_keys", IntegerType(), False),
        StructField("sample_avg_compression", FloatType(), False),
        StructField("processing_mode", StringType(), False)
    ])


def create_distributed_bda_deserializer():
    """
    Create distributed scientific deserializer compatibility function.
    
    Handles multidimensional complex arrays for distributed processing
    with complete scientific implementation.
    
    Returns
    -------
    function
        Distributed scientific deserialization function
    """
    
    def apply_scientific_distributed_deserialization(kafka_df):
        """
        Applies distributed scientific deserialization to Kafka DataFrame.
        
        Parameters
        ----------
        kafka_df : DataFrame
            Spark DataFrame with 'value' column containing MessagePack
            
        Returns  
        -------
        DataFrame
            Expanded DataFrame with complete scientific rows
        """
        logger = logging.getLogger(__name__)
        logger.info("Applying distributed scientific deserialization...")
        
        # Use advanced scientific deserializer
        scientific_deserializer_udf = create_scientific_deserializer_udf()
        
        # Deserialize and expand with complete scientific schema
        result_df = (kafka_df
                    .withColumn("scientific_rows", scientific_deserializer_udf(col("value")))
                    .select(explode(col("scientific_rows")).alias("scientific_row"))
                    .select("scientific_row.*"))
        
        logger.info("Distributed scientific deserialization completed")
        return result_df
    
    return apply_scientific_distributed_deserialization


def deserialize_msgpack_complete(raw_data_bytes: bytes) -> dict:
    """
    Deserialize complete MessagePack with numpy array reconstruction.
    
    Complete deserialization that reconstructs all scientific arrays,
    unlike consumer UDF that only returns metadata.
    
    Parameters
    ----------
    raw_data_bytes : bytes
        Serialized MessagePack data
        
    Returns
    -------
    dict
        Deserialized chunk with reconstructed numpy arrays
    """
    try:
        # Deserialize MessagePack
        msgpack_data = msgpack.unpackb(raw_data_bytes, raw=False, strict_map_key=False)
        
        chunk = {}
        for key, value in msgpack_data.items():
            if isinstance(value, dict) and 'type' in value:
                if value['type'] == 'ndarray_compressed':
                    # Decompress and reconstruct array
                    decompressed = zlib.decompress(value['data'])
                    array = np.frombuffer(decompressed, dtype=value['dtype'])
                    if 'shape' in value:
                        array = array.reshape(value['shape'])
                    chunk[key] = array
                elif value['type'] == 'ndarray':
                    # Uncompressed array
                    array = np.frombuffer(value['data'], dtype=value['dtype'])
                    if 'shape' in value:
                        array = array.reshape(value['shape'])
                    chunk[key] = array
                else:
                    chunk[key] = value
            else:
                chunk[key] = value
        
        return chunk
        
    except Exception as e:
        return {'error': str(e), 'chunk_id': 'unknown'}


def create_normalized_baseline_key(antenna1: int, antenna2: int, subms_id: str = None) -> str:
    """
    Create normalized baseline key compatible with consumer service.
    
    Ensures consistency between distributed deserialization and consumer logic.
    
    Parameters
    ----------
    antenna1 : int
        First antenna identifier
    antenna2 : int
        Second antenna identifier
    subms_id : str, optional
        SubMS identifier
        
    Returns
    -------
    str
        Normalized baseline key string
    """
    # Normalize order: always min-max
    ant_min, ant_max = sorted([antenna1, antenna2])
    
    if subms_id and subms_id != 'unknown':
        return f"{ant_min}-{ant_max}-{subms_id}"
    else:
        return f"{ant_min}-{ant_max}"


def create_distributed_bda_group_processor(bda_config: dict):
    """
    Create Pandas UDF for distributed BDA processing per group.
    
    Implements truly distributed BDA where each group (baseline, scan) is
    processed independently on a Spark worker using existing BDA scientific logic.
    
    Parameters
    ----------
    bda_config : dict
        BDA configuration parameters for processing
        
    Returns
    -------
    function
        Pandas UDF that processes visibility groups with BDA
    """
    
    @pandas_udf(returnType=get_bda_result_schema())
    def bda_group_processor(group_df):
        """
        Processes a group (baseline, scan) with BDA on distributed worker.
        
        This function executes on each Spark worker and applies BDA to a specific
        group of visibilities using existing scientific functions.
        
        Parameters
        ----------
        group_df : pd.DataFrame
            Pandas DataFrame with all group rows
            
        Returns
        -------
        pd.DataFrame
            DataFrame with BDA results for the group
        """
        start_time = time.time()
        
        try:
            if group_df.empty:
                return create_empty_bda_result()
            
            # Extract group information
            first_row = group_df.iloc[0]
            group_key = first_row['group_key']
            baseline_key = first_row['baseline_key']
            scan_number = first_row['scan_number']
            antenna1 = first_row['antenna1']
            antenna2 = first_row['antenna2']
            
            # Logging moved to debug level to avoid spam in production
            logging.debug(f"Processing BDA group: {group_key} ({len(group_df)} rows)")
            
            # Convert pandas DataFrame to BDA-compatible format
            visibility_rows = convert_pandas_to_bda_format(group_df)
            
            # Apply existing BDA logic
            try:
                # Use existing BDA processor function
                bda_results = process_microbatch_with_bda(visibility_rows, bda_config)
                
                if not bda_results:
                    # Fallback: create result without averaging
                    return create_fallback_bda_result_df(group_key, baseline_key, scan_number, antenna1, antenna2, len(group_df))
                
            except Exception as e:
                logging.warning(f"BDA processing failed for {group_key}: {e}")
                return create_fallback_bda_result_df(group_key, baseline_key, scan_number, antenna1, antenna2, len(group_df))
            
            # Convert BDA results to Spark format
            spark_results = convert_bda_results_to_spark(
                bda_results, group_key, baseline_key, scan_number, 
                antenna1, antenna2, len(group_df), start_time
            )
            
            processing_time = (time.time() - start_time) * 1000
            logging.debug(f"BDA group {group_key} completed in {processing_time:.1f}ms")
            
            return spark_results
            
        except Exception as e:
            logging.error(f"Error processing BDA group: {e}")
            return create_error_bda_result(e, group_key if 'group_key' in locals() else 'unknown')
    
    return bda_group_processor


def convert_pandas_to_bda_format(group_df) -> list:
    """
    Convert pandas DataFrame group to format expected by BDA processor.
    
    Transforms scientific rows from pandas DataFrame to the list of dictionaries
    format expected by existing BDA functions.
    
    Parameters
    ----------
    group_df : pd.DataFrame
        Pandas DataFrame with group rows
        
    Returns
    -------
    list
        List of dictionaries compatible with bda_processor
    """
    rows = []
    
    try:
        for idx, row in group_df.iterrows():
            # Reconstruct numpy arrays from Spark format
            vis_complex = reconstruct_complex_array(
                row.get('visibility_real', [[]]),
                row.get('visibility_imag', [[]]),
                row.get('visibility_shape', [0, 0])
            )
            
            weight_array = reconstruct_real_array(
                row.get('weight', [[]]),
                row.get('weight_shape', [0, 0])
            )
            
            flag_array = reconstruct_flag_array(
                row.get('flag', [[]]),
                row.get('flag_shape', [0, 0])
            )
            
            # Create dictionary compatible with bda_processor
            bda_row = {
                'baseline_key': row.get('baseline_key', 'unknown'),
                'antenna1': int(row.get('antenna1', 0)),
                'antenna2': int(row.get('antenna2', 0)),
                'scan_number': int(row.get('scan_number', 0)),
                'time': float(row.get('time', 0.0)),
                'u': float(row.get('u', 0.0)),
                'v': float(row.get('v', 0.0)),
                'w': float(row.get('w', 0.0)),
                'visibility': vis_complex,
                'weight': weight_array,
                'flag': flag_array,
                'baseline_length': float(row.get('baseline_length', 0.0)),
                'subms_id': str(row.get('subms_id', 'unknown')),
                'field_id': int(row.get('field_id', -1)),
                'spw_id': int(row.get('spw_id', -1)),
            }
            
            rows.append(bda_row)
            
    except Exception as e:
        logging.error(f"Error converting pandas to BDA format: {e}")
        
    return rows


def reconstruct_complex_array(real_list, imag_list, shape_list):
    """
    Reconstruct complex numpy array from Spark real/imag lists.
    
    Parameters
    ----------
    real_list : list
        2D list with real part
    imag_list : list
        2D list with imaginary part
    shape_list : list
        Original array shape
        
    Returns
    -------
    np.ndarray
        Reconstructed complex array
    """
    try:
        if not real_list or not imag_list or not shape_list:
            return np.array([[0.0+0.0j]])
        
        # Convert to numpy arrays
        real_array = np.array(real_list, dtype=float)
        imag_array = np.array(imag_list, dtype=float)
        
        # Combine into complex array
        complex_array = real_array + 1j * imag_array
        
        # Reshape if necessary
        if len(shape_list) >= 2 and shape_list[0] > 0 and shape_list[1] > 0:
            complex_array = complex_array.reshape(tuple(shape_list))
        
        return complex_array
        
    except Exception:
        return np.array([[0.0+0.0j]])


def reconstruct_real_array(array_list, shape_list):
    """
    Reconstruct real numpy array from Spark list.
    
    Parameters
    ----------
    array_list : list
        2D list with real values
    shape_list : list
        Original array shape
        
    Returns
    -------
    np.ndarray
        Reconstructed real array
    """
    try:
        if not array_list or not shape_list:
            return np.array([[1.0]])
        
        # Convert to numpy array
        real_array = np.array(array_list, dtype=float)
        
        # Reshape if necessary
        if len(shape_list) >= 2 and shape_list[0] > 0 and shape_list[1] > 0:
            real_array = real_array.reshape(tuple(shape_list))
        
        return real_array
        
    except Exception:
        return np.array([[1.0]])


def reconstruct_flag_array(flag_list, shape_list):
    """
    Reconstruct flag array from Spark integer list.
    
    Parameters
    ----------
    flag_list : list
        2D list with flags as integers (0/1)
    shape_list : list
        Original array shape
        
    Returns
    -------
    np.ndarray
        Reconstructed boolean flag array
    """
    try:
        if not flag_list or not shape_list:
            return np.array([[False]])
        
        # Convert to boolean numpy array
        flag_array = np.array(flag_list, dtype=bool)
        
        # Reshape if necessary
        if len(shape_list) >= 2 and shape_list[0] > 0 and shape_list[1] > 0:
            flag_array = flag_array.reshape(tuple(shape_list))
        
        return flag_array
        
    except Exception:
        return np.array([[False]])


def convert_bda_results_to_spark(bda_results, group_key, baseline_key, scan_number, 
                                antenna1, antenna2, input_rows, start_time):
    """
    Convert BDA results to pandas DataFrame format for Spark.
    
    Parameters
    ----------
    bda_results : list
        Results from BDA processor
    group_key : str
        Group key identifier
    baseline_key : str
        Baseline key identifier
    scan_number : int
        Scan number
    antenna1, antenna2 : int
        Baseline antenna identifiers
    input_rows : int
        Number of input rows
    start_time : float
        Processing start timestamp
        
    Returns
    -------
    pd.DataFrame
        Pandas DataFrame with BDA results
    """
    
    try:
        if not bda_results:
            return create_fallback_bda_result_df(
                group_key, baseline_key, scan_number, antenna1, antenna2, input_rows
            )
        
        # Take first BDA result (may have multiple windows)
        result = bda_results[0] if isinstance(bda_results, list) else bda_results
        
        # Calculate statistics
        n_windows = len(bda_results) if isinstance(bda_results, list) else 1
        compression_ratio = input_rows / n_windows if n_windows > 0 else 1.0
        processing_time = (time.time() - start_time) * 1000
        
        # Extract averaged arrays
        vis_avg_real, vis_avg_imag = extract_averaged_visibility(result)
        weight_total = extract_averaged_weight(result)
        flag_combined = extract_combined_flags(result)
        
        # Create result DataFrame
        result_data = {
            'group_key': [group_key],
            'baseline_key': [baseline_key],
            'scan_number': [scan_number],
            'antenna1': [antenna1],
            'antenna2': [antenna2],
            
            # Averaged coordinates
            'time_avg': [result.get('time_avg', result.get('time', 0.0))],
            'u_avg': [result.get('u_avg', result.get('u', 0.0))],
            'v_avg': [result.get('v_avg', result.get('v', 0.0))],
            'w_avg': [result.get('w_avg', result.get('w', 0.0))],
            'baseline_length': [result.get('baseline_length', 0.0)],
            
            # Scientific arrays
            'visibility_avg_real': [vis_avg_real],
            'visibility_avg_imag': [vis_avg_imag],
            'weight_total': [weight_total],
            'flag_combined': [flag_combined],
            
            # BDA statistics
            'n_input_rows': [int(input_rows)],
            'n_windows_created': [n_windows],
            'window_duration_s': [result.get('window_dt_s', 0.0)],
            'max_averaging_time_s': [result.get('delta_t_max', 0.0)],
            'compression_ratio': [compression_ratio],
            
            # Metadata
            'processing_timestamp': [time.time()],
            'bda_config_hash': ['distributed_v1'],
        }
        
        return pd.DataFrame(result_data)
        
    except Exception as e:
        logging.warning(f"Error converting BDA results: {e}")
        return create_fallback_bda_result_df(
            group_key, baseline_key, scan_number, antenna1, antenna2, input_rows
        )


def extract_averaged_visibility(result):
    """
    Extract averaged visibility from BDA result.
    
    Parameters
    ----------
    result : dict
        BDA processing result
        
    Returns
    -------
    tuple
        Tuple containing real and imaginary parts as lists
    """
    try:
        vis_avg = result.get('visibility_averaged', np.array([[0.0+0.0j]]))
        if np.iscomplexobj(vis_avg):
            return vis_avg.real.tolist(), vis_avg.imag.tolist()
        else:
            return [[0.0]], [[0.0]]
    except:
        return [[0.0]], [[0.0]]


def extract_averaged_weight(result):
    """
    Extract total weight from BDA result.
    
    Parameters
    ----------
    result : dict
        BDA processing result
        
    Returns
    -------
    list
        Weight array as nested list
    """
    try:
        weight = result.get('weight_total', np.array([[1.0]]))
        return weight.tolist()
    except:
        return [[1.0]]


def extract_combined_flags(result):
    """
    Extract combined flags from BDA result.
    
    Parameters
    ----------
    result : dict
        BDA processing result
        
    Returns
    -------
    list
        Combined flags as nested integer list
    """
    try:
        flags = result.get('flag_combined', np.array([[False]]))
        return flags.astype(int).tolist()
    except:
        return [[0]]


def create_fallback_bda_result_df(group_key, baseline_key, scan_number, antenna1, antenna2, input_rows):
    """
    Create fallback result when BDA processing fails.
    
    Parameters
    ----------
    group_key : str
        Group key identifier
    baseline_key : str
        Baseline key identifier
    scan_number : int
        Scan number
    antenna1, antenna2 : int
        Antenna identifiers
    input_rows : int
        Number of input rows
        
    Returns
    -------
    pd.DataFrame
        Fallback DataFrame with default values
    """
    
    return pd.DataFrame({
        'group_key': [group_key],
        'baseline_key': [baseline_key], 
        'scan_number': [scan_number],
        'antenna1': [antenna1],
        'antenna2': [antenna2],
        'time_avg': [0.0],
        'u_avg': [0.0],
        'v_avg': [0.0],
        'w_avg': [0.0],
        'baseline_length': [0.0],
        'visibility_avg_real': [[[0.0]]],
        'visibility_avg_imag': [[[0.0]]],
        'weight_total': [[[1.0]]],
        'flag_combined': [[[0]]],
        'n_input_rows': [int(input_rows)],
        'n_windows_created': [1],
        'window_duration_s': [0.0],
        'max_averaging_time_s': [0.0],
        'compression_ratio': [1.0],
        'processing_timestamp': [time.time()],
        'bda_config_hash': ['fallback'],
    })


def create_empty_bda_result():
    """
    Create empty result for groups without data.
    
    Returns
    -------
    pd.DataFrame
        Empty DataFrame with default structure
    """
    
    return pd.DataFrame({
        'group_key': ['empty'],
        'baseline_key': ['empty'],
        'scan_number': [0],
        'antenna1': [0],
        'antenna2': [0],
        'time_avg': [0.0],
        'u_avg': [0.0],
        'v_avg': [0.0],
        'w_avg': [0.0],
        'baseline_length': [0.0],
        'visibility_avg_real': [[[0.0]]],
        'visibility_avg_imag': [[[0.0]]],
        'weight_total': [[[1.0]]],
        'flag_combined': [[[0]]],
        'n_input_rows': [0],
        'n_windows_created': [0],
        'window_duration_s': [0.0],
        'max_averaging_time_s': [0.0],
        'compression_ratio': [1.0],
        'processing_timestamp': [time.time()],
        'bda_config_hash': ['empty'],
    })


def create_error_bda_result(error, group_key):
    """
    Create error result for debugging.
    
    Parameters
    ----------
    error : Exception
        The error that occurred
    group_key : str
        Group key identifier
        
    Returns
    -------
    pd.DataFrame
        Error DataFrame with error information
    """
    
    return pd.DataFrame({
        'group_key': [group_key],
        'baseline_key': ['error'],
        'scan_number': [0],
        'antenna1': [0],
        'antenna2': [0],
        'time_avg': [0.0],
        'u_avg': [0.0],
        'v_avg': [0.0],
        'w_avg': [0.0],
        'baseline_length': [0.0],
        'visibility_avg_real': [[[0.0]]],
        'visibility_avg_imag': [[[0.0]]],
        'weight_total': [[[1.0]]],
        'flag_combined': [[[0]]],
        'n_input_rows': [0],
        'n_windows_created': [0],
        'window_duration_s': [0.0],
        'max_averaging_time_s': [0.0],
        'compression_ratio': [1.0],
        'processing_timestamp': [time.time()],
        'bda_config_hash': [f'error:{str(error)[:50]}'],
    })

